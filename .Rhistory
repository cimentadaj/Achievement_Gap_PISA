enframe() %>%
unnest()
}
sample_tables_topbottom <- sample_size_calc(adapted_year_data, c(.1, .9), selected = TRUE, countries)
sample_tables_topmid <- sample_size_calc(adapted_year_data, c(.5, .9), selected = TRUE, countries)
sample_tables_midbottom <- sample_size_calc(adapted_year_data, c(.1, .5), selected = TRUE, countries)
summary_table <-
left_join(summary_data, sample_tables_topbottom) %>%
filter(name %in% c("2000", "2015")) %>%
arrange(country, name)
table_coming <-
summary_table %>%
select(-perc) %>%
gather(cats, vals, -(name:escs_dummy)) %>%
unite(all_vals, escs_dummy, cats, sep = "_") %>%
spread(all_vals, vals) %>%
map_at(c(3, 4, 6, 7), ~ {
.x[is.na(.x)] <- 0
paste0(round(.x, 0), "%")
}) %>%
map_if(is_double, as.character) %>%
data.frame() %>%
xtable::xtable()
addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- paste("\\hline \\
& & & Low SES & & & High SES \\\\
\\cmidrule(l){3-5}
\\cmidrule(l){6-8}",
paste0(
c("Year",
"Countries",
"\\% Low Edu",
"\\% High Edu",
"Sample size",
"\\% Low Edu",
"\\% High Edu",
"Sample size"),
collapse = " & "),
"\\\\ \\hline")
print(table_coming,
type = "latex",
add.to.row = addtorow,
include.colnames = FALSE,
include.rownames = FALSE,
hline.after = NULL,
caption.placement = "top",
caption = "SES sample size and ISCED composition")
# Chunk 12: merge_math_read
# Function does a lot of things, but in short:
# Calculate the difference between the gap and together with it's joint s.e
# Also uncertainty intervals and returns a tibble with the difference between
# SES gaps with the adjusted SE difference + uncertainty intervals + the original
# data (the absolute numbers before the differences)
pisa_preparer <- function(df_math, df_read) {
descrip_math <- map(df_math, ~ rename(.x, mean_math = Mean, se_math = s.e.))
descrip_read <- map(df_read, ~ rename(.x, mean_read = Mean, se_read = s.e.))
reduced_data_math <-
map2(descrip_math, years, function(.x, .y) {
.x %>%
mutate(wave = .y) %>%
filter(!is.na(escs_dummy))
}) %>%
bind_rows() %>%
as_tibble() %>%
mutate(lower_math = mean_math - 1.96 * se_math,
upper_math = mean_math + 1.96 * se_math)
reduced_data_read <-
map2(descrip_read, years, function(.x, .y) {
.x %>%
mutate(wave = .y) %>%
filter(!is.na(escs_dummy))
}) %>%
bind_rows() %>%
as_tibble() %>%
mutate(lower_read = mean_read - 1.96 * se_read,
upper_read = mean_read + 1.96 * se_read)
reduced_data <- left_join(reduced_data_math,
reduced_data_read, by = c("country", "escs_dummy", "wave"))
# Merging math and reading data
test_data <-
reduced_data %>%
select(country, wave, escs_dummy, contains("mean")) %>%
gather(test, score, contains("mean"))
math_data <-
reduced_data %>%
select(country, wave, escs_dummy, contains("math")) %>%
gather(test_bound, bound, contains("lower"), contains("upper")) %>%
select(-contains("math")) %>%
right_join(filter(test_data, test == "mean_math"))
read_data <-
reduced_data %>%
select(country, wave, escs_dummy, contains("read")) %>%
gather(test_bound, bound, contains("lower"), contains("upper")) %>%
select(-contains("read")) %>%
right_join(filter(test_data, test == "mean_read"))
all_data <- bind_rows(math_data, read_data)
# Calculate the joint standard error of the difference
math_se_data <-
reduced_data %>%
select(country, escs_dummy, wave, se_math) %>%
spread(escs_dummy, se_math) %>%
transmute(country, wave,
se_diff_math = sqrt(abs(`1`^2 - `0`^2)))
read_se_data <-
reduced_data %>%
select(country, escs_dummy, wave, se_read) %>%
spread(escs_dummy, se_read) %>%
transmute(country, wave,
se_diff_read = sqrt(abs(`1`^2 - `0`^2)))
se_data <- left_join(math_se_data, read_se_data)
# Calculate the different between the gap and together with it's joint s.e graph
# the absolut difference.
math_diff <-
reduced_data %>%
select(wave, country, escs_dummy, mean_math) %>%
spread(escs_dummy, mean_math) %>%
transmute(wave, country, diff_math = `1` - `0`)
read_diff <-
reduced_data %>%
select(wave, country, escs_dummy, mean_read) %>%
spread(escs_dummy, mean_read) %>%
transmute(wave, country, diff_read = `1` - `0`)
data_summaries <-
math_diff %>%
left_join(read_diff) %>%
left_join(se_data) %>%
transmute(wave, country, diff_math, diff_read,
lower_math = diff_math - 1.96 * se_diff_math,
lower_read = diff_read - 1.96 * se_diff_read,
upper_math = diff_math + 1.96 * se_diff_math,
upper_read = diff_read + 1.96 * se_diff_read)
differences <-
data_summaries %>%
select(wave, country, diff_math, diff_read) %>%
gather(test, difference, starts_with("diff")) %>%
mutate(type_test = ifelse(.$test == "diff_math", "math", "read"))
bounds_lower <-
data_summaries %>%
select(wave, country, contains("lower")) %>%
gather(lower_bound, lower, lower_math, lower_read) %>%
mutate(type_test = ifelse(grepl("math", .$lower_bound), "math", "read"))
bounds_upper <-
data_summaries %>%
select(wave, country, contains("upper")) %>%
gather(upper_bound, upper, upper_math, upper_read) %>%
mutate(type_test = ifelse(grepl("math", .$upper_bound), "math", "read"))
# Getting the original data in
original_math <-
reduced_data_math %>%
select(wave, everything(), -se_math) %>%
gather(metric, value, -(wave:escs_dummy)) %>%
unite(combination, escs_dummy, metric, sep = "_") %>%
spread(combination, value) %>%
mutate(type_test = "math")
original_read <-
reduced_data_read %>%
select(wave, everything(), -se_read) %>%
gather(metric, value, -(wave:escs_dummy)) %>%
unite(combination, escs_dummy, metric, sep = "_") %>%
spread(combination, value) %>%
mutate(type_test = "read")
# final data
complete_data <-
left_join(differences, bounds_lower) %>%
left_join(bounds_upper) %>%
left_join(original_math) %>%
left_join(original_read)
}
complete_data_topbottom <- pisa_preparer(results_math, results_read)
complete_data_topmid <- pisa_preparer(results_math_topmid, results_read_topmid)
complete_data_midbottom <- pisa_preparer(results_math_midbottom, results_read_midbottom)
complete_data_topbottom <- mutate(complete_data_topbottom, type = "90th/10th SES gap")
complete_data_topmid <- mutate(complete_data_topmid, type = "90th/50th SES gap")
complete_data_midbottom <- mutate(complete_data_midbottom, type = "50th/10th SES gap")
# Chunk 13: correlation_incomeineq
complete_data_topbottom %>%
mutate(wave = as.character(wave)) %>%
left_join(inequalityintsvy::economic_inequality, by = c("wave" = "year", "country")) %>%
filter(indicators == "GINI") %>%
group_by(country) %>%
summarize(avg_diff = mean(difference, na.rm = T),
avg_value = mean(value, na.rm = T)) %>%
filter(avg_value < 8) %>%
ggplot(aes(avg_value, avg_diff)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ splines::ns(x, 2), linetype = "longdash", se = F)
# Chunk 14: graphing_9010gaps
# 90/10 gaps acros countries
diff_increase_fun <- function(df) {
# Average standard deviation increase
data_ready <-
df %>%
select(wave, country, type_test, difference) %>%
group_by(type_test) %>%
split(.$country) %>%
map(~ {
.x <-
spread(.x, wave, difference) %>%
ungroup()
year_vars <- sum(map_dbl(.x, is.numeric)) - 1
years_subtract <- names(.x)[c(ncol(.x) - year_vars, ncol(.x))]
years_subtract <- lapply(years_subtract, as.name)
last_year <- rlang::new_quosure(years_subtract[[2]], env = .GlobalEnv)
first_year <- rlang::new_quosure(years_subtract[[1]], env = .GlobalEnv)
years_available <-
.x %>%
gather(year, val, -(country:type_test)) %>%
group_by(type_test) %>%
summarise(yr_avaible = sum(!is.na(val))) %>%
pull(yr_avaible)
year_sd <-
.x %>%
gather(year, val, -(country:type_test)) %>%
split(.$type_test) %>%
map_dbl(~ bootstrapper(.x, mad(val, na.rm = T), B = 100) %>% .[[2, 2]]) %>%
round(2) * 100
.x %>%
map_if(is_double, round, 3) %>%
as_tibble() %>%
transmute(type_test,
country,
diff = round(((!!last_year) - (!!first_year)), 1),
sd_year = year_sd,
diff_lower = diff - 1 * year_sd,
diff_upper = diff + 1 * year_sd,
years_available = years_available)
})
data_ready
}
diff_data <-
diff_increase_fun(complete_data_topbottom) %>%
enframer("country") %>%
filter(country %in% countries) %>%
select(country, type_test, diff) %>%
split(.$type_test) %>%
map(~ .x %>% select(-type_test) %>% deframe())
lm_data <- function(df) {
lm(log(difference) ~ wave, data = df) %>%
broom::tidy() %>%
mutate(estimate = exp(estimate))
}
ordered_cnt <-
complete_data_topbottom %>%
filter(type_test == "math") %>%
select(wave, country, difference) %>%
filter(country %in% countries) %>%
split(.$country) %>%
map(lm_data) %>%
enframer("country") %>%
filter(term == "wave") %>%
arrange(-estimate) %>%
pull(country)
complete_data_topbottom %>%
filter(country %in% countries) %>%
mutate(country = factor(country, levels = ordered_cnt, ordered = TRUE)) %>%
ggplot(aes(as.factor(wave), difference, group = type_test, colour = type_test)) +
geom_line(stat = "smooth", method = "lm", aes(group = 1),
formula = y ~ splines::ns(x, 3), linetype = "longdash",
colour = "black") +
geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.1, alpha = 0.4) +
geom_hline(yintercept = 0, linetype = "longdash") +
geom_line(alpha = 0.4) +
geom_point(size = 0.5, alpha = 0.4) +
coord_cartesian(ylim = c(0, 3)) +
facet_wrap(~ country)
# Chunk 15
# You left off here. Plot the gap for 1 and 0 for all countries in the graph before.
complete_data_topbottom %>%
filter(country %in% countries) %>%
mutate(country = factor(country, levels = ordered_cnt, ordered = TRUE)) %>%
select(wave, country, matches("*._math$")) %>%
gather(ses, gap_size, matches("^\\d_mean_math$")) %>%
separate(ses, c("ses", "delete"), sep = 2) %>%
mutate(ses = gsub("_", "", ses)) %>%
filter(complete.cases(.)) %>%
# gather(lower_bound, lower, matches("\\d_lower")) %>%
# gather(upper_bound, upper, matches("\\d_upper"))
ggplot(aes(as.factor(wave), gap_size, group = ses, colour = ses)) +
geom_point(size = 0.5, alpha = 0.4) +
geom_line(alpha = 0.4) +
geom_line(stat = "smooth", method = "lm", aes(group = 1),
formula = y ~ splines::ns(x, 1), linetype = "longdash",
colour = "black") +
# geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.1, alpha = 0.4) +
# geom_hline(yintercept = 0, linetype = "longdash") +
# coord_cartesian(ylim = c(0, 3)) +
facet_wrap(~ country)
# Chunk 16
complete_data_topbottom %>%
mutate(country = factor(country, levels = ordered_cnt, ordered = TRUE)) %>%
filter(type_test == "math", !is.na(country)) %>%
ggplot(aes(as.character(wave), difference)) +
geom_point() +
geom_linerange(aes(ymin = 0, ymax = difference))+
geom_line(stat = "smooth", method = "lm", aes(group = 1),
formula = y ~ splines::ns(x, 2), size = 0.7,
colour = "red") +
facet_wrap(~ country, ncol = 12) +
viridis::scale_fill_viridis(option = "B") +
scale_y_continuous(expand = c(0, 0), lim = c(0, 3)) +
ggthemes::theme_few() +
theme(panel.spacing = unit(1, "lines"),
axis.text.x = element_text(angle = 90, hjust = 1))
# Chunk 17: graphing_allgaps
# Comparing all gaps across countries
complete_data_topbottom %>%
bind_rows(complete_data_topmid) %>%
bind_rows(complete_data_midbottom) %>%
filter(country %in% c("United States", "Denmark", "France", "Germany")) %>%
mutate(type = factor(type,
levels = c("90th/10th SES gap", "90th/50th SES gap", "50th/10th SES gap"),
ordered = TRUE)) %>%
ggplot(aes(as.factor(wave), difference, group = type_test, colour = type_test)) +
geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.1) +
geom_hline(yintercept = 0, linetype = "longdash") +
geom_line() +
geom_point(size = 0.5) +
coord_cartesian(ylim = c(-0.5, 3)) +
facet_grid(country ~ type)
# Chunk 18: graphing_ses_growth
# Graphing how the top/bottom are evolving over time instead of absolute difference
complete_data_topbottom %>%
bind_rows(complete_data_topmid) %>%
bind_rows(complete_data_midbottom) %>%
filter(country %in% c("United States", "Denmark", "France", "Germany"), type_test == "math") %>%
select(wave, country, type_test, type, contains("math")) %>%
mutate(type = factor(type,
levels = c("90th/10th SES gap", "90th/50th SES gap", "50th/10th SES gap"),
ordered = TRUE)) %>%
gather(score, value, -(wave:type)) %>%
separate(score, c("ses", "score"), sep = 2) %>%
spread(score, value) %>%
ggplot(aes(as.factor(wave), mean_math, group = ses, colour = ses)) +
geom_errorbar(aes(ymin = lower_math, ymax = upper_math), width = 0.1) +
geom_line() +
geom_point(size = 0.5) +
coord_cartesian(ylim = c(-0.5, 3)) +
facet_grid(country ~ type)
# Increase:
# Sweden - steady increase in both tests
# Austria - increase in math - slight increase in read
# Finland - very sharp increase in both
# France - very sharp increase in both
# Netherlands - sharp increase in both
# Decrease:
# US - decrease in both tests
# Chile - decrease in both tests
# No change:
# Canada - stable red - increase math
# UK - slight decrease red - stable math
# Belgium - no change
# Czech republic - no change
# Denmark no change
# Germany - no change
# ITaly - no change
# Japan -  no change
# Norway - no change
# Poland - no change
# Spain - no change
# Chunk 19: graphing_achievement_disparity
country_scores <-
map(adapted_year_data, ~ {
if (unique(.x$wave) == "pisa2015") {
intsvy::pisa2015.mean.pv("MATH", by = "country", data = .x)
} else {
intsvy::pisa.mean.pv("MATH", by = "country", data = .x)
}
}) %>% setNames(seq(2000, 2015, 3))
ls()
country_scores <-
map(adapted_year_data, ~ {
if (unique(.x$wave) == "pisa2015") {
intsvy::pisa2015.mean.pv("MATH", by = "country", data = .x)
} else {
intsvy::pisa.mean.pv("MATH", by = "country", data = .x)
}
}) %>% setNames(seq(2000, 2015, 3))
country_scores <-
enframe(country_scores, name = "wave") %>%
unnest() %>%
mutate(wave = as.double(wave))
data_to_plot <-
left_join(complete_data_topbottom, country_scores, by = c("wave", "country")) %>%
select(wave, country, type_test, difference, Mean) %>%
filter(type_test == "math")
data_to_plot %>%
ggplot(aes(scale(Mean), difference)) +
geom_point() +
geom_smooth()
avg_increase_fun <- function(df, class) {
# Average standard deviation increase
data_ready <-
df %>%
select(wave, country, type_test, contains("mean_math")) %>%
gather(metric, value, -(wave:type_test)) %>%
separate(metric, c("ses", "test"), sep = 2) %>%
spread(test, value) %>%
mutate(ses = gsub("_", "", ses)) %>%
filter(type_test == "math", ses == class) %>%
split(.$country) %>%
map(~ mutate(.,
diff = c(diff(mean_math, lag = 1), NA),
perc = round(diff / mean_math, 2) * 100,
perc_pos = mean(perc > 0, na.rm = T))) %>%
enframe() %>%
unnest(value) %>%
split(.$country)
map2(data_ready, names(data_ready), ~ {
print(.y)
mean_df <-
bootstrapper(.x, mean(diff, na.rm = T), B = 500) %>%
filter(type == "bootstrap") %>%
rename(mean = value)
sd_df <-
bootstrapper(.x, sd(diff, na.rm = T), B = 500) %>%
filter(type == "bootstrap") %>%
rename(sd = value)
suppressMessages(
left_join(mean_df, sd_df) %>%
mutate(lower_bound = mean - 1 * sd,
upper_bound = mean + 1 * sd)
)
}) %>%
enframe() %>%
unnest(value)
}
avg_sd_increase_high <- avg_increase_fun(complete_data_topbottom, 1)
avg_sd_increase_low <- avg_increase_fun(complete_data_topbottom, 0)
avg_increase_fun <- function(df, class) {
# Average standard deviation increase
data_ready <-
df %>%
select(wave, country, type_test, contains("mean_math")) %>%
gather(metric, value, -(wave:type_test)) %>%
separate(metric, c("ses", "test"), sep = 2) %>%
spread(test, value) %>%
mutate(ses = gsub("_", "", ses)) %>%
filter(type_test == "math", ses == class) %>%
split(.$country) %>%
map(~ mutate(.,
diff = c(diff(mean_math, lag = 1), NA),
perc = round(diff / mean_math, 2) * 100,
perc_pos = mean(perc > 0, na.rm = T))) %>%
enframe() %>%
unnest(value) %>%
split(.$country)
map2(data_ready, names(data_ready), ~ {
print(.y)
mean_df <-
bootstrapper(.x, mean(diff, na.rm = T), B = 500) %>%
filter(type == "bootstrap") %>%
rename(mean = value)
sd_df <-
bootstrapper(.x, sd(diff, na.rm = T), B = 500) %>%
filter(type == "bootstrap") %>%
rename(sd = value)
suppressMessages(
left_join(mean_df, sd_df) %>%
mutate(lower_bound = mean - 1 * sd,
upper_bound = mean + 1 * sd)
)
}) %>%
enframe() %>%
unnest(value)
}
avg_sd_increase_high <- avg_increase_fun(complete_data_topbottom, 1)
avg_sd_increase_low <- avg_increase_fun(complete_data_topbottom, 0)
perc_increase_fun <- function(df) {
# Average standard deviation increase
data_ready <-
df %>%
select(wave, country, type_test, difference) %>%
group_by(type_test) %>%
split(.$country) %>%
map(~ {
.x <-
spread(.x, wave, difference) %>%
ungroup()
year_vars <- sum(map_dbl(.x, is.numeric)) - 1
years_subtract <- names(.x)[c(ncol(.x) - year_vars, ncol(.x))]
years_subtract <- lapply(years_subtract, as.name)
last_year <- rlang::new_quosure(years_subtract[[2]], env = .GlobalEnv)
first_year <- rlang::new_quosure(years_subtract[[1]], env = .GlobalEnv)
years_available <-
.x %>%
gather(year, val, -(country:type_test)) %>%
group_by(type_test) %>%
summarise(yr_avaible = sum(!is.na(val))) %>%
pull(yr_avaible)
year_sd <-
.x %>%
gather(year, val, -(country:type_test)) %>%
split(.$type_test) %>%
map_dbl(~ bootstrapper(.x, mad(val, na.rm = T), B = 100) %>% .[[2, 2]]) %>%
round(2) * 100
.x %>%
map_if(is_double, round, 3) %>%
as_tibble() %>%
transmute(type_test,
country,
perc_diff = round(((!!last_year) - (!!first_year)) * 100, 1),
sd_year = year_sd,
diff_lower = perc_diff - 1 * year_sd,
diff_upper = perc_diff + 1 * year_sd,
years_available = years_available)
})
data_ready
}
top_bottom_perc <- perc_increase_fun(complete_data_topbottom)
top_mid_perc <- perc_increase_fun(complete_data_topmid)
mid_bottom_perc <- perc_increase_fun(complete_data_midbottom)
