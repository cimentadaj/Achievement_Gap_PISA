by = c("country", "schoolid", "stidstd"))
message(paste(unique(.x$wave), "done"))
data_trend
})
pisa_all2$value[[6]] <-
pisa_all2$value[[6]] %>%
rename(escs_trend = ESCS)
# Chunk 6: functions_for_modelling
# Function calculates the bottom 30th quantile for the bottom educated and the 70th quantile
# for the top educated. If the quantiles can't be estimated, it returns two NA's instead
quantile_missing <- function(df, weights, probs) {
quan <- try(Hmisc::wtd.quantile(
df$escs_trend,
weights = df[[weights]],
probs = probs
))
if (any("try-error" %in% class(quan))) {
return(c(NA, NA))
} else {
return(c(quan[1], quan[2]))
}
}
# Producing the plot to get the difference between the top 30% of the high educated
# vs the bottom 30% of the low educated. This function loops through each dataset/country
# and survey reliability and estimates the difference while also extracting the s.e. of each
# difference.
# It returns a dataframe for each survey with all countries and respective coefficients and
# standard errors.
test_diff <- function(df, reliability, test, probs) {
map2(df, reliability, function(.x, .y) {
conf <- if (unique(.x$wave) == "pisa2015") pisa2015_conf else pisa_conf
weights_var <- conf$variables$weightFinal
country_split <- split(.x, .x$country)
country_list <- map(country_split, function(country) {
print(unique(country$country))
quan <- quantile_missing(country, weights_var, probs)
# It's very important to create a variable that returns the number of observations of this dummy
# For each country. Possibly to weight by the number of observations.
country$escs_dummy <-
with(country, case_when(escs_trend >= quan[2] ~ 1,
escs_trend <= quan[1] ~ 0))
country
})
.x <-
enframe(country_list) %>%
unnest(value)
.x <-
.x %>%
dplyr::select(wave,
matches(paste0("^PV.*", test, "$")),
escs_dummy,
country,
one_of(weights_var),
AGE)
message(paste(unique(.x$wave), "data ready"))
test_vars <- paste0("PV", seq_len(conf$parameters$PVreps), test)
.x[test_vars] <- map(.x[test_vars], ~ ifelse(.x == 9997, NA, .x))
# Calculate median math score of all PV's
.x$dv <- apply(.x[test_vars], 1, median, na.rm = T)
# Should I estimate the model separately by country?
mod1 <- lm(dv ~ AGE,
weights = .x[[weights_var]],
data = .x,
na.action = "na.exclude")
# Take residuals of model and divide by rmse. Multiply that by
# 1 / sqrt(reliability of each survey), which is .y in the loop.
.x$adj_pvnum <- resid(mod1)/rmse(mod1, .x) * 1 / sqrt(.y)
mod2 <-
lmer(adj_pvnum ~ escs_dummy + (1 + escs_dummy | country),
data = .x,
weights = .x[[weights_var]])
# Take the country coefficients (absolute coefficients)
country_coef <-
coef(mod2)$country %>%
rownames_to_column() %>%
gather(escs_dummy, Mean, -rowname) %>%
mutate(escs_dummy = dplyr::recode(escs_dummy,
`(Intercept)` = "0",
`escs_dummy` = "1"))
# Take the absolute country standard errors
se <-
se.coef(mod2)$country %>%
as.data.frame() %>%
rownames_to_column() %>%
gather(escs_dummy, s.e., -rowname) %>%
mutate(escs_dummy = dplyr::recode(escs_dummy,
`(Intercept)` = "0",
`escs_dummy` = "1"))
results <-
inner_join(country_coef, se, by = c("rowname", "escs_dummy")) %>%
rename(country = rowname) %>%
arrange(country, escs_dummy)
message(paste0(unique(.x$wave), " modeling done"))
results
})
}
# Adapted from: https://github.com/jtleek/slipper/blob/master/R/slipper.R
# Returns a tibble with the actual expr + the bootstrapped expr.
bootstrapper <- function(df, expr, B = 100, n = nrow(df), replacement = TRUE) {
bootstrapper_(df, lazyeval::lazy(expr), B, n, replacement)
}
bootstrapper_ <- function(df, expr, B = 500, n = nrow(df), replacement = TRUE) {
obs_val = lazyeval::lazy_eval(expr, data = df)
boot_val = replicate(B, {
newdata = sample_n(df, n, replace = replacement)
lazyeval::lazy_eval(expr, data = newdata)
})
out = tibble(type = c("observed", "bootstrap"),
value = c(obs_val, mean(boot_val, na.rm = T)))
return(out)
}
# For example
# bootstrapper(mtcars, mean(mpg), B = 200)
# Chunk 7: modeling
adapted_year_data <-
map(pisa_all2$value, ~ {
if (unique(.x$wave) == "pisa2000") {
# pisa2000 has a different coding so here I recode 6 to 7 so that in all waves the top edu
# is 7 and the bottom is 1
.x <-
mutate(.x, new_hisced = as.character(dplyr::recode(as.numeric(high_edu_broad), `6` = 7)))
} else {
.x <-
mutate(.x, new_hisced = as.character(high_edu_broad))
}
.x
})
# results_math <- test_diff(adapted_year_data, reliability_pisa, "MATH", c(0.1, 0.9))
# results_read <- test_diff(adapted_year_data, reliability_pisa, "READ", c(0.1, 0.9))
# results_math_topmid <- test_diff(adapted_year_data, reliability_pisa, "MATH", c(0.5, 0.9))
# results_read_topmid <- test_diff(adapted_year_data, reliability_pisa, "READ", c(0.5, 0.9))
# results_math_midbottom <- test_diff(adapted_year_data, reliability_pisa, "MATH", c(0.1, 0.5))
# results_read_midbottom <- test_diff(adapted_year_data, reliability_pisa, "READ", c(0.1, 0.5))
results_math <- read_rds("./data/delete.Rdata")
results_read <- read_rds("./data/delete_read.Rdata")
results_math_topmid <- read_rds("./data/delete_math_topmid.Rdata")
results_read_topmid <- read_rds("./data/delete_read_topmid.Rdata")
results_math_midbottom <- read_rds("./data/delete_math_midbottom.Rdata")
results_read_midbottom <- read_rds("./data/delete_read_midbottom.Rdata")
# US is missing for reading
# Cache is not working properly for the code above, so I just load the saved cached file
# load("./paper/cache/modeling_9a0b38d1d53fa243b0242580f0672fa5.RData")
# Chunk 8: sample_size
# Get sample counts for each dummy
sample_size_calc <- function(df, probs, selected = F, cnts = NULL) {
stopifnot(selected & !is.null(cnts))
if (selected) df <- map(df, ~ filter(.x, country %in% cnts))
cnt_to_bind <-
map(df, function(df) {
print(unique(df$wave))
conf <- if (unique(df$wave) == "pisa2015") pisa2015_conf else pisa_conf
weights_var <- conf$variables$weightFinal
split_df <- split(df, df$country)
split_df_two <-
map(split_df, ~ {
# In some countries the quan can't be estimated because of very few obs.
# The function doesn't stop but returns two NA's.
quan <- quantile_missing(.x, weights_var, probs)
# It's very important to create a variable that returns the number of observations of this dummy
# For each country. Possibly to weight by the number of observations.
.x$escs_dummy <-
with(.x, case_when(escs_trend >= quan[2] ~ 1,
escs_trend <= quan[1] ~ 0))
.x
})
unsplit_df <- split_df_two %>% enframe() %>% unnest(value)
unsplit_df %>%
count(country, escs_dummy) %>%
filter(!is.na(escs_dummy)) %>%
left_join(summarize(group_by(unsplit_df, country), total_n = n()), by = "country") %>%
mutate(perc = paste0(round(n / total_n * 100, 0), "%")) %>%
select(-total_n)
})
setNames(cnt_to_bind, seq(2000, 2015, 3)) %>%
enframe() %>%
unnest()
}
sample_tables_topbottom <- sample_size_calc(adapted_year_data, c(.1, .9), selected = TRUE, countries)
sample_tables_topmid <- sample_size_calc(adapted_year_data, c(.5, .9), selected = TRUE, countries)
sample_tables_midbottom <- sample_size_calc(adapted_year_data, c(.1, .5), selected = TRUE, countries)
# Chunk 9: merge_math_read
# Function does a lot of things, but in short:
# Calculate the difference between the gap and together with it's joint s.e
# Also uncertainty intervals and returns a tibble with the difference between
# SES gaps with the adjusted SE difference + uncertainty intervals + the original
# data (the absolute numbers before the differences)
pisa_preparer <- function(df_math, df_read) {
descrip_math <- map(df_math, ~ rename(.x, mean_math = Mean, se_math = s.e.))
descrip_read <- map(df_read, ~ rename(.x, mean_read = Mean, se_read = s.e.))
reduced_data_math <-
map2(descrip_math, years, function(.x, .y) {
.x %>%
mutate(wave = .y) %>%
filter(!is.na(escs_dummy))
}) %>%
bind_rows() %>%
as_tibble() %>%
mutate(lower_math = mean_math - 1.96 * se_math,
upper_math = mean_math + 1.96 * se_math)
reduced_data_read <-
map2(descrip_read, years, function(.x, .y) {
.x %>%
mutate(wave = .y) %>%
filter(!is.na(escs_dummy))
}) %>%
bind_rows() %>%
as_tibble() %>%
mutate(lower_read = mean_read - 1.96 * se_read,
upper_read = mean_read + 1.96 * se_read)
reduced_data <- left_join(reduced_data_math,
reduced_data_read, by = c("country", "escs_dummy", "wave"))
# Merging math and reading data
test_data <-
reduced_data %>%
select(country, wave, escs_dummy, contains("mean")) %>%
gather(test, score, contains("mean"))
math_data <-
reduced_data %>%
select(country, wave, escs_dummy, contains("math")) %>%
gather(test_bound, bound, contains("lower"), contains("upper")) %>%
select(-contains("math")) %>%
right_join(filter(test_data, test == "mean_math"))
read_data <-
reduced_data %>%
select(country, wave, escs_dummy, contains("read")) %>%
gather(test_bound, bound, contains("lower"), contains("upper")) %>%
select(-contains("read")) %>%
right_join(filter(test_data, test == "mean_read"))
all_data <- bind_rows(math_data, read_data)
# Calculate the joint standard error of the difference
math_se_data <-
reduced_data %>%
select(country, escs_dummy, wave, se_math) %>%
spread(escs_dummy, se_math) %>%
transmute(country, wave,
se_diff_math = sqrt(abs(`1`^2 - `0`^2)))
read_se_data <-
reduced_data %>%
select(country, escs_dummy, wave, se_read) %>%
spread(escs_dummy, se_read) %>%
transmute(country, wave,
se_diff_read = sqrt(abs(`1`^2 - `0`^2)))
se_data <- left_join(math_se_data, read_se_data)
# Calculate the different between the gap and together with it's joint s.e graph
# the absolut difference.
math_diff <-
reduced_data %>%
select(wave, country, escs_dummy, mean_math) %>%
spread(escs_dummy, mean_math) %>%
transmute(wave, country, diff_math = `1` - `0`)
read_diff <-
reduced_data %>%
select(wave, country, escs_dummy, mean_read) %>%
spread(escs_dummy, mean_read) %>%
transmute(wave, country, diff_read = `1` - `0`)
data_summaries <-
math_diff %>%
left_join(read_diff) %>%
left_join(se_data) %>%
transmute(wave, country, diff_math, diff_read,
lower_math = diff_math - 1.96 * se_diff_math,
lower_read = diff_read - 1.96 * se_diff_read,
upper_math = diff_math + 1.96 * se_diff_math,
upper_read = diff_read + 1.96 * se_diff_read)
differences <-
data_summaries %>%
select(wave, country, diff_math, diff_read) %>%
gather(test, difference, starts_with("diff")) %>%
mutate(type_test = ifelse(.$test == "diff_math", "math", "read"))
bounds_lower <-
data_summaries %>%
select(wave, country, contains("lower")) %>%
gather(lower_bound, lower, lower_math, lower_read) %>%
mutate(type_test = ifelse(grepl("math", .$lower_bound), "math", "read"))
bounds_upper <-
data_summaries %>%
select(wave, country, contains("upper")) %>%
gather(upper_bound, upper, upper_math, upper_read) %>%
mutate(type_test = ifelse(grepl("math", .$upper_bound), "math", "read"))
# Getting the original data in
original_math <-
reduced_data_math %>%
select(wave, everything(), -se_math) %>%
gather(metric, value, -(wave:escs_dummy)) %>%
unite(combination, escs_dummy, metric, sep = "_") %>%
spread(combination, value) %>%
mutate(type_test = "math")
original_read <-
reduced_data_read %>%
select(wave, everything(), -se_read) %>%
gather(metric, value, -(wave:escs_dummy)) %>%
unite(combination, escs_dummy, metric, sep = "_") %>%
spread(combination, value) %>%
mutate(type_test = "read")
# final data
complete_data <-
left_join(differences, bounds_lower) %>%
left_join(bounds_upper) %>%
left_join(original_math) %>%
left_join(original_read)
}
complete_data_topbottom <- pisa_preparer(results_math, results_read)
complete_data_topmid <- pisa_preparer(results_math_topmid, results_read_topmid)
complete_data_midbottom <- pisa_preparer(results_math_midbottom, results_read_midbottom)
complete_data_topbottom <- mutate(complete_data_topbottom, type = "90th/10th SES gap")
complete_data_topmid <- mutate(complete_data_topmid, type = "90th/50th SES gap")
complete_data_midbottom <- mutate(complete_data_midbottom, type = "50th/10th SES gap")
# Chunk 10: correlation_incomeineq
complete_data_topbottom %>%
mutate(wave = as.character(wave)) %>%
left_join(inequalityintsvy::economic_inequality, by = c("wave" = "year", "country")) %>%
filter(indicators == "GINI") %>%
group_by(country) %>%
summarize(avg_diff = mean(difference, na.rm = T),
avg_value = mean(value, na.rm = T)) %>%
filter(avg_value < 8) %>%
ggplot(aes(avg_value, avg_diff)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ splines::ns(x, 2), linetype = "longdash", se = F)
# Chunk 11: graphing_9010gaps
# 90/10 gaps acros countries
complete_data_topbottom %>%
filter(country %in% c("United States", "Netherlands", "France",
"Germany", "Poland", "Finland")) %>%
ggplot(aes(as.factor(wave), difference, group = type_test, colour = type_test)) +
geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.1) +
geom_hline(yintercept = 0, linetype = "longdash") +
geom_line() +
geom_point(size = 0.5) +
coord_cartesian(ylim = c(-0.5, 3)) +
facet_wrap(~ country)
# Chunk 12: graphing_allgaps
# Comparing all gaps across countries
complete_data_topbottom %>%
bind_rows(complete_data_topmid) %>%
bind_rows(complete_data_midbottom) %>%
filter(country %in% c("United States", "Denmark", "France")) %>%
mutate(type = factor(type,
levels = c("90th/10th SES gap", "90th/50th SES gap", "50th/10th SES gap"),
ordered = TRUE)) %>%
ggplot(aes(as.factor(wave), difference, group = type_test, colour = type_test)) +
geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.1) +
geom_hline(yintercept = 0, linetype = "longdash") +
geom_line() +
geom_point(size = 0.5) +
coord_cartesian(ylim = c(-0.5, 3)) +
facet_grid(country ~ type)
# Chunk 13: graphing_ses_growth
# Graphing how the top/bottom are evolving over time instead of absolute difference
complete_data_topbottom %>%
bind_rows(complete_data_topmid) %>%
bind_rows(complete_data_midbottom) %>%
filter(country %in% c("Germany", "Denmark", "France"), type_test == "math") %>%
select(wave, country, type_test, type, contains("math")) %>%
mutate(type = factor(type,
levels = c("90th/10th SES gap", "90th/50th SES gap", "50th/10th SES gap"),
ordered = TRUE)) %>%
gather(score, value, -(wave:type)) %>%
separate(score, c("ses", "score"), sep = 2) %>%
spread(score, value) %>%
ggplot(aes(as.factor(wave), mean_math, group = ses, colour = ses)) +
geom_errorbar(aes(ymin = lower_math, ymax = upper_math), width = 0.1) +
geom_hline(yintercept = 0, linetype = "longdash") +
geom_line() +
geom_point(size = 0.5) +
coord_cartesian(ylim = c(-0.5, 3)) +
facet_grid(country ~ type)
# Increase:
# Sweden - steady increase in both tests
# Austria - increase in math - slight increase in read
# Finland - very sharp increase in both
# France - very sharp increase in both
# Netherlands - sharp increase in both
# Decrease:
# US - decrease in both tests
# Chile - decrease in both tests
# No change:
# Canada - stable red - increase math
# UK - slight decrease red - stable math
# Belgium - no change
# Czech republic - no change
# Denmark no change
# Germany - no change
# ITaly - no change
# Japan -  no change
# Norway - no change
# Poland - no change
# Spain - no change
# Chunk 14: rate_change
avg_increase_fun <- function(df, class) {
# Average standard deviation increase
data_ready <-
df %>%
select(wave, country, type_test, contains("mean_math")) %>%
gather(metric, value, -(wave:type_test)) %>%
separate(metric, c("ses", "test"), sep = 2) %>%
spread(test, value) %>%
mutate(ses = gsub("_", "", ses)) %>%
filter(type_test == "math", ses == class) %>%
split(.$country) %>%
map(~ mutate(.,
diff = c(diff(mean_math, lag = 1), NA),
perc = round(diff / mean_math, 2) * 100,
perc_pos = mean(perc > 0, na.rm = T))) %>%
enframe() %>%
unnest(value) %>%
split(.$country)
map2(data_ready, names(data_ready), ~ {
print(.y)
mean_df <-
bootstrapper(.x, mean(diff, na.rm = T), B = 500) %>%
filter(type == "bootstrap") %>%
rename(mean = value)
sd_df <-
bootstrapper(.x, sd(diff, na.rm = T), B = 500) %>%
filter(type == "bootstrap") %>%
rename(sd = value)
suppressMessages(
left_join(mean_df, sd_df) %>%
mutate(lower_bound = mean - 1 * sd,
upper_bound = mean + 1 * sd)
)
}) %>%
enframe() %>%
unnest(value)
}
avg_sd_increase_high <- avg_increase_fun(complete_data_topbottom, 1)
avg_sd_increase_low <- avg_increase_fun(complete_data_topbottom, 0)
# Chunk 15: rate_change_graph
full_data <-
left_join(select(avg_sd_increase_high, name, mean),
select(avg_sd_increase_low, name, mean), by = "name") %>%
mutate(continent = ifelse(name %in% countries, "my_cnt", "other_cnt"))
colnames(full_data) <- c("country", "high_increase", "low_increase", "continent")
lims <- list(xlim = c(-0.15, 0.25), ylim = c(-0.25, 0.25))
rect_data <- tibble(xst = c(lims$xlim[1], 0),
xen = c(0.0, lims$xlim[2]),
yst = c(0.0, lims$ylim[1]),
yen = c(lims$ylim[2], 0),
colour = c("red", "green"))
full_data %>%
ggplot(aes(low_increase, high_increase), alpha = 0.2) +
geom_rect(data = rect_data, aes(xmin = xst,
xmax = xen,
ymin = yst,
ymax = yen),
fill = rect_data$colour,
alpha = 0.2,
inherit.aes = FALSE) +
geom_line(stat="smooth", method = "lm", se = FALSE, alpha = 0.5, colour = "grey", size = 1) +
geom_point(alpha = 0.2) +
geom_point(data = filter(full_data, continent == "my_cnt"), colour = "red", alpha = 0.7) +
geom_text_repel(data = filter(full_data, continent == "my_cnt"),
aes(label = country), box.padding = unit(2.7, "lines")) +
geom_vline(xintercept = 0, alpha = 0.5) +
geom_hline(yintercept = 0, alpha = 0.5) +
xlim(lims$xlim) +
ylim(lims$ylim) +
coord_cartesian(expand = FALSE) +
annotate(geom = "text", x = 0.15, y = -0.2,
label = "Low SES are catching up \n faster than High SES",
fontface = 2, size = 3) +
annotate(geom = "text", x = -0.05, y = 0.20,
label = "High SES are increasing  \n faster than Low SES",
fontface = 2, size = 3) +
labs(x = "Average increase of low SES in SD", y = "Average increase of high SES in SD") +
theme_minimal()
# Chunk 16: perc_increase_tables
# Show the rates at which is increasing/decreasing
perc_increase_fun <- function(df) {
# Average standard deviation increase
data_ready <-
df %>%
select(wave, country, type_test, difference) %>%
group_by(type_test) %>%
split(.$country) %>%
map(~ {
.x <-
spread(.x, wave, difference) %>%
ungroup()
year_vars <- sum(map_dbl(.x, is.numeric)) - 1
years_subtract <- names(.x)[c(ncol(.x) - year_vars, ncol(.x))]
years_subtract <- lapply(years_subtract, as.name)
last_year <- rlang::new_quosure(years_subtract[[2]], env = .GlobalEnv)
first_year <- rlang::new_quosure(years_subtract[[1]], env = .GlobalEnv)
years_available <-
.x %>%
gather(year, val, -(country:type_test)) %>%
group_by(type_test) %>%
summarise(yr_avaible = sum(!is.na(val))) %>%
pull(yr_avaible)
year_sd <-
.x %>%
gather(year, val, -(country:type_test)) %>%
split(.$type_test) %>%
map_dbl(~ bootstrapper(.x, mad(val, na.rm = T), B = 100) %>% .[[2, 2]]) %>%
round(2) * 100
.x %>%
transmute(type_test,
country,
perc_diff = round(((!!last_year) - (!!first_year)) / (!!first_year) * 100, 1),
sd_year = year_sd,
diff_lower = perc_diff - 1 * year_sd,
diff_upper = perc_diff + 1 * year_sd,
years_available = years_available)
})
data_ready
}
top_bottom_perc <- perc_increase_fun(complete_data_topbottom)
top_mid_perc <- perc_increase_fun(complete_data_topmid)
mid_bottom_perc <- perc_increase_fun(complete_data_midbottom)
# Gap is closing at an average of the variable diff per year.
