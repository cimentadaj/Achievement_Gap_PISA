"New Zealand",
"Austria",
"Australia",
"Sweden",
"Czech Republic",
"Canada",
"Hungary",
"Iceland", "Netherlands",
"Spain",
"Belgium",
"Italy", "Norway",
"United Kingdom",
"Greece",
"Denmark",
"Israel",
"Poland",
"United States",
"Germany",
"Turkey",
"Russia")
# Chunk 3: loading_data-recoding
pisa_all <- read_rds("./data/pisa_listcol.Rdata")
pisa_all2 <- pisa_all
years <- seq(2000, 2015, 3)
db <- paste0("pisa", years)
pisa_all2$value <- map2(pisa_all2$value, db, ~ { .x$wave <- .y; .x})
pisa_all2$value[[1]]$CNT <- pisa_all2$value[[1]]$COUNTRY
pisa_all2$value <- map(pisa_all2$value, ~ {
# 2000 to 2015
# The coding is from 0 to 6, where 0 is no schooling and 6 is
# BA or above.
# When turning 0:6 to numeric, it becomes 1:7 that's why
# I recode 8:9 to NA. This, however, didn't work for last two surveys
.x$father_edu <- car::recode(as.numeric(.x$FISCED), "8:9 = NA")
.x$mother_edu <- car::recode(as.numeric(.x$MISCED), "8:9 = NA")
.x$high_edu_broad <- pmax(.x$father_edu, .x$mother_edu)
.x$country <- pisa_countrynames[as.character(.x$CNT)]
if (any(unique(.x$wave) %in% c("pisa2012", "pisa2015"))) {
# These two surveys were from 0:6 so I had to add + 1
# so that it equals 1:7 as all other surveys.
.x$father_edu <- .x$father_edu + 1
.x$mother_edu <- .x$mother_edu + 1
.x$high_edu_broad <- .x$high_edu_broad + 1
}
.x
})
reliability_pisa <-
c("2000" = 0.81,
"2003" = 0.85,
"2006" = 0.78,
"2009" = 0.74,
"2012" = 0.82,
"2015" = 0.74) # 2015 imputed
# Chunk 4: escs_trend
# Rescaled trend ESCS data to merge.
# This only has data for seq(2000, 2012, 3) because
# PISA 2015 has the ESCS trend variable.
dir <- tempdir()
file_name <- "escs_trend.zip"
download.file("http://vs-web-fs-1.oecd.org/pisa/trend_escs_SPSS.zip",
destfile = file.path(dir, file_name))
unzip(file.path(dir, file_name), exdir = dir)
escs_trend <- map(file.path(dir, list.files(dir, pattern = ".sav")), haven::read_spss)
file.remove(file.path(dir, list.files(dir)))
escs_trend <-
map(escs_trend, ~ {
mutate(.x, cnt = pisa_countrynames[cnt]) %>%
rename(country = cnt)
})
# Chunk 5: merge_escs_pisa
# Next we'll merge the ESCS data with the PISA data. As explained above, the 6th data (PISA
# 2015) doesn't need to be merged so I exclude it with this vector
exclude <- -6
# Loop in parallel to the PISA data, the ESCS data and the year vector (which is seq(2012, 2015, 3))
pisa_all2$value[exclude] <-
pmap(list(pisa_all2$value[exclude], escs_trend, years[exclude]), function(.x, .y, .z) {
# The escs data needs to have the key variables the same class as the
# same data.
escs <-
.y %>% mutate(schoolid = as.numeric(schoolid),
stidstd = as.numeric(stidstd))
# .z is the corresponding year that will be created as a column
# And perform the same transformation of the key variables as in the ESCS data
data_trend <-
.x %>%
mutate(
year = .z,
schoolid = as.numeric(as.character(SCHOOLID)),
stidstd = as.numeric(as.character(STIDSTD))
) %>%
left_join(escs,
by = c("country", "schoolid", "stidstd"))
message(paste(unique(.x$wave), "done"))
data_trend
})
pisa_all2$value[[6]] <-
pisa_all2$value[[6]] %>%
rename(escs_trend = ESCS)
# Chunk 6: functions_for_modelling
# Function calculates the bottom 30th quantile for the bottom educated and the 70th quantile
# for the top educated. If the quantiles can't be estimated, it returns two NA's instead
quantile_missing <- function(df, weights, probs) {
quan <- try(Hmisc::wtd.quantile(
df$escs_trend,
weights = df[[weights]],
probs = probs
))
if (any("try-error" %in% class(quan))) {
return(c(NA, NA))
} else {
return(c(quan[1], quan[2]))
}
}
# Producing the plot to get the difference between the top 30% of the high educated
# vs the bottom 30% of the low educated. This function loops through each dataset/country
# and survey reliability and estimates the difference while also extracting the s.e. of each
# difference.
# It returns a dataframe for each survey with all countries and respective coefficients and
# standard errors.
test_diff <- function(df, reliability, test, probs) {
map2(df, reliability, function(.x, .y) {
conf <- if (unique(.x$wave) == "pisa2015") pisa2015_conf else pisa_conf
weights_var <- conf$variables$weightFinal
country_split <- split(.x, .x$country)
country_list <- map(country_split, function(country) {
print(unique(country$country))
quan <- quantile_missing(country, weights_var, probs)
# It's very important to create a variable that returns the number of observations of this dummy
# For each country. Possibly to weight by the number of observations.
country$escs_dummy <-
with(country, case_when(escs_trend >= quan[2] ~ 1,
escs_trend <= quan[1] ~ 0))
country
})
.x <-
enframe(country_list) %>%
unnest(value)
.x <-
.x %>%
dplyr::select(wave,
matches(paste0("^PV.*", test, "$")),
escs_dummy,
country,
one_of(weights_var),
AGE)
message(paste(unique(.x$wave), "data ready"))
test_vars <- paste0("PV", seq_len(conf$parameters$PVreps), test)
.x[test_vars] <- map(.x[test_vars], ~ ifelse(.x == 9997, NA, .x))
# Calculate median math score of all PV's
.x$dv <- apply(.x[test_vars], 1, median, na.rm = T)
# Should I estimate the model separately by country?
mod1 <- lm(dv ~ AGE,
weights = .x[[weights_var]],
data = .x,
na.action = "na.exclude")
# Take residuals of model and divide by rmse. Multiply that by
# 1 / sqrt(reliability of each survey), which is .y in the loop.
.x$adj_pvnum <- resid(mod1)/rmse(mod1, .x) * 1 / sqrt(.y)
mod2 <-
lmer(adj_pvnum ~ escs_dummy + (1 + escs_dummy | country),
data = .x,
weights = .x[[weights_var]])
# Take the country coefficients (absolute coefficients)
country_coef <-
coef(mod2)$country %>%
rownames_to_column() %>%
gather(escs_dummy, Mean, -rowname) %>%
mutate(escs_dummy = dplyr::recode(escs_dummy,
`(Intercept)` = "0",
`escs_dummy` = "1"))
# Take the absolute country standard errors
se <-
se.coef(mod2)$country %>%
as.data.frame() %>%
rownames_to_column() %>%
gather(escs_dummy, s.e., -rowname) %>%
mutate(escs_dummy = dplyr::recode(escs_dummy,
`(Intercept)` = "0",
`escs_dummy` = "1"))
results <-
inner_join(country_coef, se, by = c("rowname", "escs_dummy")) %>%
rename(country = rowname) %>%
arrange(country, escs_dummy)
message(paste0(unique(.x$wave), " modeling done"))
results
})
}
# Adapted from: https://github.com/jtleek/slipper/blob/master/R/slipper.R
# Returns a tibble with the actual expr + the bootstrapped expr.
bootstrapper <- function(df, expr, B = 100, n = nrow(df), replacement = TRUE) {
bootstrapper_(df, lazyeval::lazy(expr), B, n, replacement)
}
bootstrapper_ <- function(df, expr, B = 500, n = nrow(df), replacement = TRUE) {
obs_val = lazyeval::lazy_eval(expr, data = df)
boot_val = replicate(B, {
newdata = sample_n(df, n, replace = replacement)
lazyeval::lazy_eval(expr, data = newdata)
})
out = tibble(type = c("observed", "bootstrap"),
value = c(obs_val, mean(boot_val, na.rm = T)))
return(out)
}
# For example
# bootstrapper(mtcars, mean(mpg), B = 200)
# Chunk 7: modeling
adapted_year_data <-
map(pisa_all2$value, ~ {
if (unique(.x$wave) == "pisa2000") {
# pisa2000 has a different coding so here I recode 6 to 7 so that in all waves the top edu
# is 7 and the bottom is 1
.x <-
mutate(.x, new_hisced = as.character(dplyr::recode(as.numeric(high_edu_broad), `6` = 7)))
} else {
.x <-
mutate(.x, new_hisced = as.character(high_edu_broad))
}
.x
})
# results_math <- test_diff(adapted_year_data, reliability_pisa, "MATH", c(0.1, 0.9))
# results_read <- test_diff(adapted_year_data, reliability_pisa, "READ", c(0.1, 0.9))
# results_math_topmid <- test_diff(adapted_year_data, reliability_pisa, "MATH", c(0.5, 0.9))
# results_read_topmid <- test_diff(adapted_year_data, reliability_pisa, "READ", c(0.5, 0.9))
# results_math_midbottom <- test_diff(adapted_year_data, reliability_pisa, "MATH", c(0.1, 0.5))
# results_read_midbottom <- test_diff(adapted_year_data, reliability_pisa, "READ", c(0.1, 0.5))
results_math <- read_rds("./data/delete.Rdata")
results_read <- read_rds("./data/delete_read.Rdata")
results_math_topmid <- read_rds("./data/delete_math_topmid.Rdata")
results_read_topmid <- read_rds("./data/delete_read_topmid.Rdata")
results_math_midbottom <- read_rds("./data/delete_math_midbottom.Rdata")
results_read_midbottom <- read_rds("./data/delete_read_midbottom.Rdata")
# US is missing for reading
# Cache is not working properly for the code above, so I just load the saved cached file
# load("./paper/cache/modeling_9a0b38d1d53fa243b0242580f0672fa5.RData")
# Chunk 8: country_sample_numeric_vec
country_rows <-
map_dbl(adapted_year_data, nrow) %>%
format(big.mark = ",")
# Chunk 9: variance_pisa
var_years <-
map(adapted_year_data, function(.x) {
.x %>%
transmute(avg_cogn = matrixStats::rowMedians(as.matrix(select(.x, matches("^PV*.MATH$"))))) %>%
summarize(var = sd(avg_cogn, na.rm = T),
var_of_var = ) %>%
pull(var)
})
enframer <- function(df, col_name = "name") {
df %>%
enframe(name = col_name) %>%
unnest()
}
var_years %>%
setNames(seq(2000, 2015, 3)) %>%
enframer(name) %>%
ggplot(aes(name, value)) + geom_col()
# Chunk 11: sample_size
# Get sample counts for each dummy
sample_size_calc <- function(df, probs, selected = F, cnts = NULL) {
stopifnot(selected & !is.null(cnts))
if (selected) df <- map(df, ~ filter(.x, country %in% cnts))
cnt_to_bind <-
map(df, function(df) {
print(unique(df$wave))
conf <- if (unique(df$wave) == "pisa2015") pisa2015_conf else pisa_conf
weights_var <- conf$variables$weightFinal
split_df <- split(df, df$country)
split_df_two <-
map(split_df, ~ {
# In some countries the quan can't be estimated because of very few obs.
# The function doesn't stop but returns two NA's.
quan <- quantile_missing(.x, weights_var, probs)
# It's very important to create a variable that returns the number of observations of this dummy
# For each country. Possibly to weight by the number of observations.
.x$escs_dummy <-
with(.x, case_when(escs_trend >= quan[2] ~ 1,
escs_trend <= quan[1] ~ 0))
.x
})
unsplit_df <- split_df_two %>% enframe() %>% unnest(value)
unsplit_df %>%
count(country, escs_dummy) %>%
filter(!is.na(escs_dummy)) %>%
left_join(summarize(group_by(unsplit_df, country), total_n = n()), by = "country") %>%
mutate(perc = paste0(round(n / total_n * 100, 0), "%")) %>%
select(-total_n)
})
setNames(cnt_to_bind, seq(2000, 2015, 3)) %>%
enframe() %>%
unnest()
}
sample_tables_topbottom <- sample_size_calc(adapted_year_data, c(.1, .9), selected = TRUE, countries)
sample_tables_topmid <- sample_size_calc(adapted_year_data, c(.5, .9), selected = TRUE, countries)
sample_tables_midbottom <- sample_size_calc(adapted_year_data, c(.1, .5), selected = TRUE, countries)
# Chunk 12: merge_math_read
# Function does a lot of things, but in short:
# Calculate the difference between the gap and together with it's joint s.e
# Also uncertainty intervals and returns a tibble with the difference between
# SES gaps with the adjusted SE difference + uncertainty intervals + the original
# data (the absolute numbers before the differences)
pisa_preparer <- function(df_math, df_read) {
descrip_math <- map(df_math, ~ rename(.x, mean_math = Mean, se_math = s.e.))
descrip_read <- map(df_read, ~ rename(.x, mean_read = Mean, se_read = s.e.))
reduced_data_math <-
map2(descrip_math, years, function(.x, .y) {
.x %>%
mutate(wave = .y) %>%
filter(!is.na(escs_dummy))
}) %>%
bind_rows() %>%
as_tibble() %>%
mutate(lower_math = mean_math - 1.96 * se_math,
upper_math = mean_math + 1.96 * se_math)
reduced_data_read <-
map2(descrip_read, years, function(.x, .y) {
.x %>%
mutate(wave = .y) %>%
filter(!is.na(escs_dummy))
}) %>%
bind_rows() %>%
as_tibble() %>%
mutate(lower_read = mean_read - 1.96 * se_read,
upper_read = mean_read + 1.96 * se_read)
reduced_data <- left_join(reduced_data_math,
reduced_data_read, by = c("country", "escs_dummy", "wave"))
# Merging math and reading data
test_data <-
reduced_data %>%
select(country, wave, escs_dummy, contains("mean")) %>%
gather(test, score, contains("mean"))
math_data <-
reduced_data %>%
select(country, wave, escs_dummy, contains("math")) %>%
gather(test_bound, bound, contains("lower"), contains("upper")) %>%
select(-contains("math")) %>%
right_join(filter(test_data, test == "mean_math"))
read_data <-
reduced_data %>%
select(country, wave, escs_dummy, contains("read")) %>%
gather(test_bound, bound, contains("lower"), contains("upper")) %>%
select(-contains("read")) %>%
right_join(filter(test_data, test == "mean_read"))
all_data <- bind_rows(math_data, read_data)
# Calculate the joint standard error of the difference
math_se_data <-
reduced_data %>%
select(country, escs_dummy, wave, se_math) %>%
spread(escs_dummy, se_math) %>%
transmute(country, wave,
se_diff_math = sqrt(abs(`1`^2 - `0`^2)))
read_se_data <-
reduced_data %>%
select(country, escs_dummy, wave, se_read) %>%
spread(escs_dummy, se_read) %>%
transmute(country, wave,
se_diff_read = sqrt(abs(`1`^2 - `0`^2)))
se_data <- left_join(math_se_data, read_se_data)
# Calculate the different between the gap and together with it's joint s.e graph
# the absolut difference.
math_diff <-
reduced_data %>%
select(wave, country, escs_dummy, mean_math) %>%
spread(escs_dummy, mean_math) %>%
transmute(wave, country, diff_math = `1` - `0`)
read_diff <-
reduced_data %>%
select(wave, country, escs_dummy, mean_read) %>%
spread(escs_dummy, mean_read) %>%
transmute(wave, country, diff_read = `1` - `0`)
data_summaries <-
math_diff %>%
left_join(read_diff) %>%
left_join(se_data) %>%
transmute(wave, country, diff_math, diff_read,
lower_math = diff_math - 1.96 * se_diff_math,
lower_read = diff_read - 1.96 * se_diff_read,
upper_math = diff_math + 1.96 * se_diff_math,
upper_read = diff_read + 1.96 * se_diff_read)
differences <-
data_summaries %>%
select(wave, country, diff_math, diff_read) %>%
gather(test, difference, starts_with("diff")) %>%
mutate(type_test = ifelse(.$test == "diff_math", "math", "read"))
bounds_lower <-
data_summaries %>%
select(wave, country, contains("lower")) %>%
gather(lower_bound, lower, lower_math, lower_read) %>%
mutate(type_test = ifelse(grepl("math", .$lower_bound), "math", "read"))
bounds_upper <-
data_summaries %>%
select(wave, country, contains("upper")) %>%
gather(upper_bound, upper, upper_math, upper_read) %>%
mutate(type_test = ifelse(grepl("math", .$upper_bound), "math", "read"))
# Getting the original data in
original_math <-
reduced_data_math %>%
select(wave, everything(), -se_math) %>%
gather(metric, value, -(wave:escs_dummy)) %>%
unite(combination, escs_dummy, metric, sep = "_") %>%
spread(combination, value) %>%
mutate(type_test = "math")
original_read <-
reduced_data_read %>%
select(wave, everything(), -se_read) %>%
gather(metric, value, -(wave:escs_dummy)) %>%
unite(combination, escs_dummy, metric, sep = "_") %>%
spread(combination, value) %>%
mutate(type_test = "read")
# final data
complete_data <-
left_join(differences, bounds_lower) %>%
left_join(bounds_upper) %>%
left_join(original_math) %>%
left_join(original_read)
}
complete_data_topbottom <- pisa_preparer(results_math, results_read)
complete_data_topmid <- pisa_preparer(results_math_topmid, results_read_topmid)
complete_data_midbottom <- pisa_preparer(results_math_midbottom, results_read_midbottom)
complete_data_topbottom <- mutate(complete_data_topbottom, type = "90th/10th SES gap")
complete_data_topmid <- mutate(complete_data_topmid, type = "90th/50th SES gap")
complete_data_midbottom <- mutate(complete_data_midbottom, type = "50th/10th SES gap")
# Chunk 13: correlation_incomeineq
complete_data_topbottom %>%
mutate(wave = as.character(wave)) %>%
left_join(inequalityintsvy::economic_inequality, by = c("wave" = "year", "country")) %>%
filter(indicators == "GINI") %>%
group_by(country) %>%
summarize(avg_diff = mean(difference, na.rm = T),
avg_value = mean(value, na.rm = T)) %>%
filter(avg_value < 8) %>%
ggplot(aes(avg_value, avg_diff)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ splines::ns(x, 2), linetype = "longdash", se = F)
# Chunk 14: graphing_9010gaps
# 90/10 gaps acros countries
diff_increase_fun <- function(df) {
# Average standard deviation increase
data_ready <-
df %>%
select(wave, country, type_test, difference) %>%
group_by(type_test) %>%
split(.$country) %>%
map(~ {
.x <-
spread(.x, wave, difference) %>%
ungroup()
year_vars <- sum(map_dbl(.x, is.numeric)) - 1
years_subtract <- names(.x)[c(ncol(.x) - year_vars, ncol(.x))]
years_subtract <- lapply(years_subtract, as.name)
last_year <- rlang::new_quosure(years_subtract[[2]], env = .GlobalEnv)
first_year <- rlang::new_quosure(years_subtract[[1]], env = .GlobalEnv)
years_available <-
.x %>%
gather(year, val, -(country:type_test)) %>%
group_by(type_test) %>%
summarise(yr_avaible = sum(!is.na(val))) %>%
pull(yr_avaible)
year_sd <-
.x %>%
gather(year, val, -(country:type_test)) %>%
split(.$type_test) %>%
map_dbl(~ bootstrapper(.x, mad(val, na.rm = T), B = 100) %>% .[[2, 2]]) %>%
round(2) * 100
.x %>%
map_if(is_double, round, 3) %>%
as_tibble() %>%
transmute(type_test,
country,
diff = round(((!!last_year) - (!!first_year)), 1),
sd_year = year_sd,
diff_lower = diff - 1 * year_sd,
diff_upper = diff + 1 * year_sd,
years_available = years_available)
})
data_ready
}
diff_data <-
diff_increase_fun(complete_data_topbottom) %>%
enframer("country") %>%
filter(country %in% countries) %>%
select(country, type_test, diff) %>%
split(.$type_test) %>%
map(~ .x %>% select(-type_test) %>% deframe())
lm_data <- function(df) {
lm(log(difference) ~ wave, data = df) %>%
broom::tidy() %>%
mutate(estimate = exp(estimate))
}
ordered_cnt <-
complete_data_topbottom %>%
filter(type_test == "math") %>%
select(wave, country, difference) %>%
filter(country %in% countries) %>%
split(.$country) %>%
map(lm_data) %>%
enframer("country") %>%
filter(term == "wave") %>%
arrange(-estimate) %>%
pull(country)
complete_data_topbottom %>%
filter(country %in% countries) %>%
mutate(country = factor(country, levels = ordered_cnt, ordered = TRUE)) %>%
ggplot(aes(as.factor(wave), difference, group = type_test, colour = type_test)) +
geom_line(stat = "smooth", method = "lm", aes(group = 1),
formula = y ~ splines::ns(x, 3), linetype = "longdash",
colour = "black") +
geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.1, alpha = 0.4) +
geom_hline(yintercept = 0, linetype = "longdash") +
geom_line(alpha = 0.4) +
geom_point(size = 0.5, alpha = 0.4) +
coord_cartesian(ylim = c(0, 3)) +
facet_wrap(~ country)
complete_data_topbottom
