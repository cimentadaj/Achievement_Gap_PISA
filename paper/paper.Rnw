\documentclass[11pt, a4paper]{article}
\bibliographystyle{apalike}
\pagestyle{headings}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
\usepackage{pdflscape}
\usepackage{subcaption}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage[round, colon]{natbib}
\usepackage[colorlinks]{hyperref}
\AtBeginDocument{%
  \hypersetup{
    citecolor=blue,
    linkcolor=blue,   
    urlcolor=blue}}

\title{First paper - Draft}
\author{Jorge Cimentada}


\begin{document}
\setlength{\parindent}{2em}
\setlength{\parskip}{1em}
\showboxdepth=5
\showboxbreadth=5

\maketitle

<<working directory, echo = F>>=
# opts_knit$set(root.dir = '..')

opts_chunk$set(echo = F,
               message = F,
               warning = F,
               include = F,
               cache.lazy = F,
               results = 'asis')
@

<<default_conf>>=
  library(knitr)
  library(arm)
  library(saves)
  library(haven)
  library(PISA2000lite)
  library(PISA2003lite)
  library(PISA2006lite)
  library(PISA2009lite)
  library(PISA2012lite)
  library(intsvy)
  library(cimentadaj) # # devtools::install_github("cimentadaj/cimentadaj")
  library(countrycode) # For region variable
  library(car)
  library(readr)
  library(SAScii)
  library(inequalityintsvy) # devtools::install_github("cimentadaj/inequalityintsvy")
  library(lme4)
  library(modelr)
  library(tidyverse)
  library(ggrepel)
  
  # source("./transform_data.R")

  # Conf for PISA_2015
  pisa2015_conf <- list(variables = list(pvlabelpref = "PV",
                                         pvlabelsuff = "READ",
                                         weightFinal = "W_FSTUWT",
                                         weightBRR = "W_FSTURWT"),
          parameters = list(cutoffs = c(357.77, 420.07, 482.38, 544.68, 606.99, 669.30),
                                          percentiles = c(5, 10, 25, 75, 90, 95),
                                          PVreps = 10,
                                          BRRreps = 80,
                                          weights = "BRR",
                                          replication_scheme = 'pisa')
  )
  
  countries <- c("Australia",
                 "Germany",
                 "Denmark",
                 "Spain",
                 "France",
                 "Italy",
                 "Netherlands",
                 "Sweden",
                 "Finland",
                 "United States",
                 "United Kingdom")
@

\tableofcontents

<<loading_data-recoding>>=
pisa_all <- read_rds("./data/pisa_listcol.Rdata")
pisa_all2 <- pisa_all

years <- seq(2000, 2015, 3)
  
db <- paste0("pisa", years)
pisa_all2$value <- map2(pisa_all2$value, db, ~ { .x$wave <- .y; .x})
pisa_all2$value[[1]]$CNT <- pisa_all2$value[[1]]$COUNTRY
  
pisa_all2$value <- map(pisa_all2$value, ~ {
  
# 2000 to 2015
# The coding is from 0 to 6, where 0 is no schooling and 6 is
# BA or above.

# When turning 0:6 to numeric, it becomes 1:7 that's why
# I recode 8:9 to NA. This, however, didn't work for last two surveys
  
  .x$father_edu <- car::recode(as.numeric(.x$FISCED), "8:9 = NA")
  .x$mother_edu <- car::recode(as.numeric(.x$MISCED), "8:9 = NA")
  .x$high_edu_broad <- pmax(.x$father_edu, .x$mother_edu)
  .x$country <- pisa_countrynames[as.character(.x$CNT)]
  
  if (any(unique(.x$wave) %in% c("pisa2012", "pisa2015"))) {
    # These two surveys were from 0:6 so I had to add + 1
    # so that it equals 1:7 as all other surveys.
    .x$father_edu <- .x$father_edu + 1
    .x$mother_edu <- .x$mother_edu + 1
    .x$high_edu_broad <- .x$high_edu_broad + 1
  }
  .x
})
  
reliability_pisa <-
  c("2000" = 0.81,
    "2003" = 0.85,
    "2006" = 0.78,
    "2009" = 0.74,
    "2012" = 0.82,
    "2015" = 0.74) # 2015 imputed

@

<<escs_trend, cache = TRUE>>=
  # Rescaled trend ESCS data to merge.
  # This only has data for seq(2000, 2012, 3) because
  # PISA 2015 has the ESCS trend variable.
  dir <- tempdir()
  file_name <- "escs_trend.zip"
  download.file("http://vs-web-fs-1.oecd.org/pisa/trend_escs_SPSS.zip",
                destfile = file.path(dir, file_name))
  unzip(file.path(dir, file_name), exdir = dir)
  escs_trend <- map(file.path(dir, list.files(dir, pattern = ".sav")), haven::read_spss)
  file.remove(file.path(dir, list.files(dir)))
  
  escs_trend <-
    map(escs_trend, ~ {
    mutate(.x, cnt = pisa_countrynames[cnt]) %>%
    rename(country = cnt)
  })
@

<<merge_escs_pisa, cache = TRUE>>=
   # Next we'll merge the ESCS data with the PISA data. As explained above, the 6th data (PISA
  # 2015) doesn't need to be merged so I exclude it with this vector
  exclude <- -6
  
  # Loop in parallel to the PISA data, the ESCS data and the year vector (which is seq(2012, 2015, 3))
  pisa_all2$value[exclude] <-
    pmap(list(pisa_all2$value[exclude], escs_trend, years[exclude]), function(.x, .y, .z) {
    
    # The escs data needs to have the key variables the same class as the
    # same data.
    escs <-
      .y %>% mutate(schoolid = as.numeric(schoolid),
                    stidstd = as.numeric(stidstd))
    
    # .z is the corresponding year that will be created as a column
    # And perform the same transformation of the key variables as in the ESCS data
    data_trend <-
      .x %>%
        mutate(
          year = .z,
          schoolid = as.numeric(as.character(SCHOOLID)),
          stidstd = as.numeric(as.character(STIDSTD))
          ) %>%
   left_join(escs,
              by = c("country", "schoolid", "stidstd"))
    
    message(paste(unique(.x$wave), "done"))
    
    data_trend
  })
  
  pisa_all2$value[[6]] <-
    pisa_all2$value[[6]] %>%
    rename(escs_trend = ESCS)
@

<<functions_for_modelling>>=

# Function calculates the bottom 30th quantile for the bottom educated and the 70th quantile
# for the top educated. If the quantiles can't be estimated, it returns two NA's instead
quantile_missing <- function(df, weights, probs) {
    
    quan <- try(Hmisc::wtd.quantile(
      df$escs_trend,
      weights = df[[weights]],
      probs = probs
      ))

    if (any("try-error" %in% class(quan))) {
      return(c(NA, NA))
      } else {
     return(c(quan[1], quan[2]))
    }
}
  
# Producing the plot to get the difference between the top 30% of the high educated
# vs the bottom 30% of the low educated. This function loops through each dataset/country
# and survey reliability and estimates the difference while also extracting the s.e. of each
# difference.
  
# It returns a dataframe for each survey with all countries and respective coefficients and
# standard errors.
test_diff <- function(df, reliability, test, probs) {
  
    map2(df, reliability, function(.x, .y) {
      
      conf <- if (unique(.x$wave) == "pisa2015") pisa2015_conf else pisa_conf
      weights_var <- conf$variables$weightFinal

      country_split <- split(.x, .x$country)
      
      country_list <- map(country_split, function(country) {
        print(unique(country$country))
        
        quan <- quantile_missing(country, weights_var, probs)
        
        # It's very important to create a variable that returns the number of observations of this dummy
        # For each country. Possibly to weight by the number of observations.
        country$escs_dummy <-
          with(country, case_when(escs_trend >= quan[2] ~ 1,
                                  escs_trend <= quan[1] ~ 0))
        country
      })
      
      .x <-
        enframe(country_list) %>%
        unnest(value)

      .x <-
        .x %>%
        dplyr::select(wave,
                      matches(paste0("^PV.*", test, "$")),
                      escs_dummy,
                      country,
                      one_of(weights_var),
                      AGE)
      
      message(paste(unique(.x$wave), "data ready"))


      test_vars <- paste0("PV", seq_len(conf$parameters$PVreps), test)
      .x[test_vars] <- map(.x[test_vars], ~ ifelse(.x == 9997, NA, .x))
      
      # Calculate median math score of all PV's
      .x$dv <- apply(.x[test_vars], 1, median, na.rm = T)
      
      # Should I estimate the model separately by country?
      mod1 <- lm(dv ~ AGE,
                 weights = .x[[weights_var]],
                 data = .x,
                 na.action = "na.exclude")
      
      # Take residuals of model and divide by rmse. Multiply that by
      # 1 / sqrt(reliability of each survey), which is .y in the loop.
      .x$adj_pvnum <- resid(mod1)/rmse(mod1, .x) * 1 / sqrt(.y)
      
      mod2 <-
        lmer(adj_pvnum ~ escs_dummy + (1 + escs_dummy | country),
             data = .x,
             weights = .x[[weights_var]])
      
      # Take the country coefficients (absolute coefficients)
      country_coef <-
        coef(mod2)$country %>%
        rownames_to_column() %>%
        gather(escs_dummy, Mean, -rowname) %>%
        mutate(escs_dummy = dplyr::recode(escs_dummy,
                                          `(Intercept)` = "0",
                                          `escs_dummy` = "1"))
      
      # Take the absolute country standard errors
      se <-
        se.coef(mod2)$country %>%
        as.data.frame() %>%
        rownames_to_column() %>%
        gather(escs_dummy, s.e., -rowname) %>%
        mutate(escs_dummy = dplyr::recode(escs_dummy,
                                          `(Intercept)` = "0",
                                          `escs_dummy` = "1"))
      
      results <-
        inner_join(country_coef, se, by = c("rowname", "escs_dummy")) %>%
        rename(country = rowname) %>%
        arrange(country, escs_dummy)
      
      message(paste0(unique(.x$wave), " modeling done"))
      results
    })
}


# Adapted from: https://github.com/jtleek/slipper/blob/master/R/slipper.R
# Returns a tibble with the actual expr + the bootstrapped expr.
bootstrapper <- function(df, expr, B = 100, n = nrow(df), replacement = TRUE) {
  bootstrapper_(df, lazyeval::lazy(expr), B, n, replacement)
}

bootstrapper_ <- function(df, expr, B = 500, n = nrow(df), replacement = TRUE) {
  obs_val = lazyeval::lazy_eval(expr, data = df)
  boot_val = replicate(B, {
    newdata = sample_n(df, n, replace = replacement)
    lazyeval::lazy_eval(expr, data = newdata)
  })
  out = tibble(type = c("observed", "bootstrap"), 
               value = c(obs_val, mean(boot_val, na.rm = T)))
  return(out)
}

# For example
# bootstrapper(mtcars, mean(mpg), B = 200)
@

% This chunk needs to have cache = TRUE once you want to run it for like 2 hours until 
% the results_* models have been created once and the cache can save them

<<modeling, cache = TRUE>>=
adapted_year_data <-
    map(pisa_all2$value, ~ {
      if (unique(.x$wave) == "pisa2000") {
        # pisa2000 has a different coding so here I recode 6 to 7 so that in all waves the top edu
        # is 7 and the bottom is 1
        .x <-
          mutate(.x, new_hisced = as.character(dplyr::recode(as.numeric(high_edu_broad), `6` = 7)))
      } else {
        .x <-
          mutate(.x, new_hisced = as.character(high_edu_broad))
      }
      .x
})

results_math <- test_diff(adapted_year_data, reliability_pisa, "MATH", c(0.1, 0.9))
results_read <- test_diff(adapted_year_data, reliability_pisa, "READ", c(0.1, 0.9))
results_math_topmid <- test_diff(adapted_year_data, reliability_pisa, "MATH", c(0.5, 0.9))
results_read_topmid <- test_diff(adapted_year_data, reliability_pisa, "READ", c(0.5, 0.9))
results_math_midbottom <- test_diff(adapted_year_data, reliability_pisa, "MATH", c(0.1, 0.5))
results_read_midbottom <- test_diff(adapted_year_data, reliability_pisa, "READ", c(0.1, 0.5))

# results_math <- read_rds("./data/delete.Rdata")
# results_read <- read_rds("./data/delete_read.Rdata")
# results_math_topmid <- read_rds("./data/delete_math_topmid.Rdata")
# results_read_topmid <- read_rds("./data/delete_read_topmid.Rdata")
# results_math_midbottom <- read_rds("./data/delete_math_midbottom.Rdata")
# results_read_midbottom <- read_rds("./data/delete_read_midbottom.Rdata")
# US is missing for reading

# Cache is not working properly for the code above, so I just load the saved cached file
# load("./paper/cache/modeling_9a0b38d1d53fa243b0242580f0672fa5.RData")
@

\section{Literature Review}

Recent research on educational inequality has found that differences in test peformance between High-SES and Low-SES kids has been growing quickly over the years. The literature on educational inequality has mainly concentrated on the United States \citep{reardon2011} but other international evidence is emerging with other similar findings for other count. The United States is usually the case study of interest, firstly, because it is the only country where cognitive testing is very widespread across surveys. And secondly, this trend has allowed to have testing records as early as 1940 (check reardon) until present day. Using this information, \citep{reardon2011} is the first to investigate the evolution of the cognitive gap and the results are very surprising. Not only has the cognitive gap between the 90th income percentile and the 10th income percentile grown over time, but it has grown faster and to be wider than the highly contested white-black gap (cite study of white-black gap -waldfogel study). The widening of the achievement gap has been happening in parallel to the growth of income inequality. Although very suggestive, it is hard to link both things causally.

\citet{reardon2011} finds that the increase has occured predominantely from the 1970's until the 2000's. In fact, hard numbers suggest that the gap increased by about ~ 40-50\%. The author also estimates the rate of change using data from the 1940's and finds an even higher increase of about ~ 75\%. Since the studies before the 1970's lack a lot of rigor in comparability and sampling, the author computes everything for the sample before/after 1970's separately \footnote{Even after weighting appropriately.}. Finally, \citet{readon2011} concludes that the U.S gap between the 90th and 10th income percentile is at about 1.25 standard deviations.  \citet{bradbury2015} don't find results which are very far away from this. They find that for 14 year olds in the United States, the gap is above 1 SD but lower than 1.25. \citet{duncan2011} find similarly to the previous studies and confirm a gap of ~ 1.25/1.50 standard deviations.

One important drawback of these studies is that they don't present the uncertainty of this estimate. Not necessarily to gauge their statistical significance, but to simply asses how much we can trust their accuracy. Despite the recent surge in understanding achievement gaps as a function of social background, sociologists have already amassed a great deal of findings concerning inter-generational inequalities, one notable example being that it's very clear that large inequalities in parent's education tend to reproduce large inequalities in children's education \citep{breen2007}. Virtually in all countries, be it developed or developing, there is inequality of opportunity. But there is considerable variation in the magnitud of this effect. For example, the Scandinavian countries, particularly Denmark and Sweden, prove to be very mobile countries \citep{esping2012, breen2007, shavit1993}.

With very recent data, \citet{reardon_portilla} have unexpectedly uncovered a new finding: the reversal of the trend. From 1998 until 2010, it seems like the income achievement gap \footnote{The income achievement gap is the difference between the 90th and 10th income percentile in the test of interest} decreased at a rate of 0.01 and 0.008 standard deviations per year. In contrast to \citep{reardon2011}, we find that in a 30-year span the gap was systematicaly increasing at a rate of 0.02, something not dramatically different from the previous estimates. The American situation is a very distinctive one relative to Europe. The reasons why we're seeing a reversal in the trend could be numerous and should be studied very close to context. For example, the American educational system lacks any formal curricular differentiation \emph{à la} European style. In contrast, \citet{reardon2011} suggest that the reversal is likely due to the high increase of preschool enrollment. They suggest that in this same period (1998 - 2010) the income achievement gap in early schooling enrollment decreased substantially.

Some authors have taken this analysis to an international context in order to discover between-country trends. The comprehensive work of \citet{bradbury2015} performs a comparative analysis of Australia, United Kingdom, United States and Canada. Their research design is very distinctive in that they use longitudinal data from children as early as age 2 and study the evolution of the achievement gap up until age 14 \footnote{To the best of my knowledge this is the only study that uses panel data to study achievement gaps, let alone to do this between countries}. The core finding behind the book is that the American achievement gap is much wider than in any other comparison countries. As mentioned earlier, they find that once the achievement gap is present in early school entry, it doesn't seem to narrow very much over the life course. They find that the achievement gap is very stable all the way from early education to secondary school (although their studies cover up until age 14). 

% You left documenting the bibliography here
One limitation of their study is that they concentrate on countries which have a very particular educational structure, namely the fact that there is little formal stratification in terms of curricula. These four countries have no major jump in school selection, quite the opposite to the European average countries. A thorough review by \citet{werfhorst_mijs} sheds some light on the subject. First and foremost, they gather substantive evidence showing that countries which have a highly tracked curriculum tend to have high levels of inequality, measured in terms of achievement gaps. Hanushek and Woessman (tracking study), use the TIMSS and PISA surveys to gauge whether highly tracked countries do indeed increase inequality after students pass the age at first selection of tracking. Moreover, Mickleweight \& Schnepf (2007) using similar data but different design finds that countries which have a high level of tracking, are distinctively unequal in the difference between the top 95th and bottom 5th performers. In fact, the difference in test scores between these two groups is about 10 times higher then the average annual gain of a year of schooling.

Despite their high level of rigurosity, their analysis is based on four surveys that have significant differences in terms of questions, sampling and populations and cannot be easily compared. Their findings are very reliable but should be taken as suggestive at best.  For this reason we should also pay particular attention to studies such as Chmielewski and Reardon (2016) and Chmielewski (2017) which have attempted not only to compare gaps between countries, but to evaluate whether there is a general increase in educational inequality in many countries using similar studies. These studies tackle a completely different question from the above, but they do provide support for the overall finding that the achievement gap is certainly not narrowing over time.

Chmielewski and Reardon (2016), using the Programme for International Student Assessment (PISA), the Trends in International Mathematics and Science Study (TIMSS) and PIRLS (the Progress in International Reading Literacy Study), the assess whether there are patterns of cross-national variation in the achievement gap. In other words, is the size of achievement gap different for many countries? Although there is potential to study countries over time using these surveys, they choose not to. They do so because their question of interest (income categories) was only asked in three waves; in the end the only had three countries repeated in all three waves. 

Their work suggests that there is considerable variation in the achievement gap between top and bottom earning families across many developed countries. In comparison to the literature on achievement gaps, they find that the U.S has a gap a little over 1 standard deviation in the year 2005 (for 15 year olds) and Germany has a decreasing gap from around ~ 1.25 to ~ 1 standard deviation. They go even further and link this achievement gap to several country-level indicators related to income inequality, school differentiation, central exams, etc ... The correlations are indeed very suggestive but clearly we must be extremely cautios in drawing causal changes from these two variables. But it is important to stress that their design is quite different from the work of Reardon (2011). Reardon (2011) takes studies in the U.S starting from the 1940's until 2015 \footnote{Each study is independent of each other meaning that the it might've been given to 6 year olds as well as to 12 year olds. Although the author adjusts for age, the trends can't be generalized to age-specific, but rather in overall terms.} and makes them comparable across time. This gives the author a very long time series to build a reliable achievement gap (over 40 years). In their study, Chmielewski and Reardon (2016) change the aim of the study to model between-country differences from a cross-sectional perspective.

Chmielewski (2017), building on the work of Chmielewski and Reardon (2016) and Reardon and Portilla (2015) pooled together all the previously mentioned data, together with over 10 more studies ranging from the year 1964 until 2015 in order to discover differences between and across countries. With over 50 years of data, and over 100 countries, Chmielewski (2017) finds that there seems to be a general world increase in the achievement gap. However, once she disentangles the relationship by country, she finds a reasonable amount of heterogeneity, with some countries seeing the achievement gap closing, others no change at all, while others record a steady increase. One clear limitation of their study (as well as Reardon (2011)) is that the adjust for the age of each child in all studies. Although for their purposes is the right thing to do \footnote{The differences in achievement could simply be due to changes in cognitive abilities across the lifetime. However, as we've noted before, Corak and Waldfogel (2015) find that the achievement gap is very stable across the life time}, they are masking age-specific achievement gaps by controlling for age, such as Reardon (2011) did.

The evolution of High/Low SES gaps for preschool children might be much less marked than the same gap for high school children. The explanation, although very debated, has been gaining much support in recent years. In countries with high levels of curricular differentiation, the transition from early schooling into the tracking system has been found to increase inequality of learning (Wossman and Hanushek diff-in-diff). Moreover, the vast sociological literature on educational transitions systematically finds that tracking tends to foster between-track inequality rather than erode their differences by tackling their specific needs (Van der Werfhorst and Mijs 2011). Based on this, we cannot simply assume that the achievement gap has been neither constant across cohorts (because there have been tracking reforms in many countries, introducing as well as elimination tracking structures) nor the same between ages, because tracking/no tracking might exarcebate the achievement gap.

With this being said, this paper introduces one novelty in the literature which is to evaluate the evolution of the High/Low SES achievement gap in the past 15 years for all PISA participant countries. This is different from previous work because it concentrates solely on 15 year old children, and it attempts to capture the evolution of the achievement gap for each country. The advantages of this study are numerous. First, we concentrate on the evolution of the gap for only 15 year olds. This is different from all the work of Chmielewski and Reardon, in which they pool all ages to get average changes, to study the properties of specific age-groups. As we've seen before, there are reasons to think that specific age-groups have seen changes in the achievement gap. Moreover, in almost all countries with a tracked curriculum children are either at or in the process of tracking by the age 15, meaning that we will be able to link whether tracked countries are the most variable in their evolution of achievement gaps.

The rule of thumb to choose the 90th, 50th and 10th percentile is arbitrary, others have used, for example, the 5th, 50th and 95th. (Micklewright \& Schnepf, 2007)

Calculate the difference between 90/10 and then the ratio of this with the annual gain of one year of schooling (Mickleweight \& Schnepf did it). They also find that France is a lower dispersion country (but in my results this is changing a lot). They also find that Germany and US are very dispersed countries (quite the opposite)

Mickleweight \& Schnepf find that France has a very small gap in 2000, something we find very clearly in our data, and the U.S has a very big gap in 2000, something we also find in our data. Mickleweight \& Schnpef find
that average achievement is negatively related to dispersion.

Is efficiency related to opportunity here? Is efficiency what explains the inequality of opportunity?

Talk about how the achievement gap has been widening in Malaysia and South Korea (or Japan?)
Note how many of these studies haven't really concentrated on who is getting better or worse: top or bottom?
Talk more about how countries with high social mobility has been linked to smallest achievement gaps

Talk about Durpiez and Dumay and how there's not relationship between inequality income - inequality achievement in contrast to reardon and anna who find some relationship.

\section{Research questions}

This paper is interested in two questions. As explained in the literature review, we already know that U.S achievement gaps have been growing over time, and some international evidence suggests that it might be increasing in other countries as well. All of these studies pool all students together from different ages and estimate global trends.

We take a different approach and we want to study age-specific trends in achievement gaps. We're interested in studying a) the size of the achievement in a comparative perspective and its relationship to overall achievement levels, b) which countries are experiencing changes in the achievement gap, c) the rate at which each country-gap is widening/narrowing and d) establishing whether the gaps are widening/narrowing because particular groups are getting ahead/behind.

We develop each question separately for more detail.

\begin{description}
  \item[We want to test the notion that better performing countries have lower leves of achievement gap. \citep{werfhorst_mijs} emphasize that there is empirical evidence that suggests this. This pattern is not so obvious. For example, countries with high leves of tracking could maximize student performance, specially the high SES students, raising their overall performance and thus raising the national performance score. But if the bottom performers are not gaining at the same rate, then the achievement gap will inevitable grow resulting in a high performing countries with widening achievement gap.]
  \item[The seminal work of \citet{reardon2011} suggests that achievement gaps change, and they do so much quicker that we though. After recording a SES gap increase of about 40\% in only 30 years, \citep{reardon_portilla} stress that they also found a significant decrease in only 15 years of data. This lends credibility that our estimations will show variation over time. As usual, their estimations are only for children going deom 4th to 8th grade. We concentrate on age-specific groups, in this case, 15 year olds.]
  \item[We want to compare the percentage change at which the gap widened/narrowed from the first to the last year. This will give us a general idea of the overall change over time, and will allow us to compare our estimates to the actual literature \footnote{Although no study has performed this age-specific achievement gap for comparable tests over such a long time. Our results will serve as comparison for other studies that use age-specific groups, such as 4th graders.}]
  \item[The widening/narrowing of the achievement gap has a source, which has been often studied to be related to everything from educational spending, income inequality, time allocation to students and preschool enrollment. The literature has concentrated very narrowly on whether the gap is increasing because the top performers are getting ahead, because both are distancing or because the bottom is falling behind. We shall pay particular attention to identifying the rate at which the top/bottom groups are evolving over time. This type of analysis is particularly useful in jotting down the mechanisms through which the gap is evolving. If the bottom performers are stagnated whereas the top is gaining ground, the efficiency-trade off dilemma of tracking might be much more credible than other explanations.]
\end{description}

\section{Methods}

\subsection{Data}

<<country_sample_numeric_vec>>=
country_rows <- map_dbl(adapted_year_data, nrow)
@

In order the study the above mentioned questions I will be using the Programme for International Student Assessment (PISA). PISA is a survey carried out every three years that aims to evaluate education systems by testing the skills and knowledge of 15-year-old students. Currrently, PISA has six waves starting in the year 2000 up until 2015, where recently, over half a million students were tested in mathematics, literacy and science in over 70 countries, both developed and developing ones.

PISA collects data through a two-stage stratified sampling design. With the help of official governments, PISA randomly chooses 150 schools in each country, where they then randomly pick thirty 15 year olds to undertake the two hour tests. The sample size for PISA 2000 is \Sexpr{country_rows[1]}, for PISA 2003, \Sexpr{country_rows[2]}, for PISA 2006, \Sexpr{country_rows[3]}, for PISA 2009, \Sexpr{country_rows[4]}, for PISA 2012, \Sexpr{country_rows[5]}, and for PISA 2015 \Sexpr{country_rows[6]}. Together with the subject tests, PISA collects personal information from students, their families and their school environment (including teacher surveys), that serves as relevent background information that can be matched to the students performance. With the recent inclusion of PISA 2015, these six waves make up a time-series analysis of 15 years, enough time to visualize changes in the structure of an educational system.

In order to measure family background PISA includes several variables. Firstly, the ask student's their parent's educational level. Scholars have considered this to be a reliable recall given that we expect fifteen year olds to know their parent's level of education (Reardon 2011). This question has been asked in every wave and holds a somewhat similar coding across time, although the first two waves have a different coding. Moreover, another limitation is the fact that parent's education is measured with the ISCED classification, something that has changed over time. For example, until PISA 2009, the ISCED 1997 classification was currently used, but the next wave already used the new ISCED 2011 classification. Both these classification schemes have equivalent look-up tables, yet this requires a detailed inspection of the codings.

Another social background variable PISA collects is the International Socio-Economic Index of Occupational Status (ISEI). This variable captures the economic status of the family, the closest it can without asking for income information. It has been scaled for comparability between waves and some authors have used it for inequality studies finding expected results to be consistent with social origin literature(Anna study about world-wide inequality). PISA also includes a plethora of indicators on family wealth, home educational resources, the number of books in the home, among many other material resources in the household.

But one of the most relevant variables for our study is the is a composite SES index created by the PISA team. The index of economic, social and cultural status (ESCS) was created on the basis of the following variables: the International Socio-Economic Index of Occupational Status (ISEI); the highest level of education of the student’s parents, converted into years of schooling; the PISA index of family wealth; the PISA index of home educational resources; and the PISA index of possessions related to "classical" culture in the family home (Education at a Glance, OECD, Paris, 2002, Glossary). This variable, aside from capturing all relevant dimensions of SES, such as education, occupation, and material resources, takes care of transforming the variables into comparable measures across waves. In fact, we use the newly-released ESCS index (% http://www.oecd-ilibrary.org/education/pisa-2015-results-volume-i/pisa-2015-technical-background_9789264266490-13-en
) which rescaled all ESCS indexes to make them suitable for over-time analysis \footnote{These rescaled indices can be found in the \href{http://www.oecd.org/pisa/data/2015database/}{PISA website} under \emph{Rescaled Indices for Trend Analyses}.}.

Aside from SES, the other most relevant variables are test scores. PISA provides plausible values instead of one single indicator of test score performance. As explained in the PISA manual (PISA 2012, 2015), these are imputed values that resemble individual test scores and have approximately the same distribution as the latent trait being measured. Suppose we have µi, the average student test score in mathematics for student i. Instead of estimating µi alone, plausible values estimate a distribution of possible µ’s, for student i, together with the likelihood of each µi based on the respondents answers on the test. This is defined as the posterior distributions of µ’s for student i. The reason why we use this procedure is because estimating a single estimate µi is plagued with measurement error, among other types of bias (see Wu, 2005). The plausible values in the PIAAC data are effectively five (ten for PISA 2015) random draws from this distribution. These variable is are continuous, ranging from 0 to 500, with a mean of 250.

\subsection{Data analysis}

The aim of this paper is to identify and disaggregate country trends in the achievement gap for several countries. To represent the SES gap, most of the literature on achievement gaps has concentrated on indicators such as parental education, parental occupational status, income achievement gaps and actual SES achievement gaps(Fryer and Levitt 2004). The actual calculation of the achievement gap varies substantially and different strategies have been implemented. For example, Mickleweight \& Funsche calculate the difference in achievement by crudely subtracting the gap between the 95th and 5th percentile of the mathematics distribution. Although in principle you should be able to capture some type of SES effect like this, theoretically, it should be much more accurate to difference out the mean score of, for example, parental education or some other SES proxy. Guan Kung Saw (2015), for instance, used parental education as a proxy of SES, whereas Soo-yong Byun and Kyung-keun Kim (2010) use a similar SES index as ours, but created by them.

Reardon (2011) and subsequently (all reardon papers), used a different method developed by Reardon (2011) which we partially adopt adopt in this paper. SES achievement gaps are measured as the difference in standardized achievement between the 90th and 10th percentiles of the chosen SES variable.

% Transform all of this text into equations

We start by standardizing achievement for each country, year and subject. Because PISA does not provide a single achievement indicator, we take the median of all plausible values for each student \footnote{Since each plausible value is a random draw from a theoretical latent normal distribution of possible student achievement scores, the median should be precise in getting a central measure of the latent distribution.}, resulting in one single score. We then fit a linear model for each wave where we regress the single achievement score on the age variable weighted by the appropriate student sample weights. We take the residuals and divide them by the root mean square error of the model and divide that by one over the square root of the reliability of the survey for that year-subject (implement this all in formula). \footnote{Other proceedures multiply each country by their own reliability measure for each year-subject pair (Anna analysis of wordwide gap). Unfortunately, PISA 2000 did not provide any reliability measure separately for each country and PISA 2015 has to yet release their own at the moment of writing this paper. For this reason we implement the analysis following the original work of Reardon (2011)}

Standardizing the median test score solves the problem of comparability of gaps measured with different tests, and across waves. However, if the variance of academic achievement changes over time, then standardizing the overall score at each wave of testing confounds changes in the gap with changes in the variance of test scores. That is, by standardizing we're forcing the variance of test scores to be zero across all waves. But if the true variance of academic achievement grows over time, then the estimated trend in the achievement gaps will be underestimated, and vice versa.

% Show graph of variance over time
% I'm showing standard deviations, is it better to show variance?
<<variance_pisa, cache = TRUE>>=

var_years <-
  map(adapted_year_data, function(.x) {
  .x %>%
  transmute(avg_cogn = matrixStats::rowMedians(as.matrix(select(.x, matches("^PV*.MATH$"))))) %>%
  summarize(var = sd(avg_cogn, na.rm = T),
            var_of_var = ) %>%
  pull(var)
})

enframer <- function(df) {
  df %>%
    enframe() %>%
    unnest()
}

var_years %>%
  setNames(seq(2000, 2015, 3)) %>%
  enframer() %>%
  ggplot(aes(name, value)) + geom_col()
@

Inspection of the data suggests that it's something we shouldn't be deeply concerned with. The standard deviation of each wave seems to be following a very similar pattern with an exception of the year 2000.

Another concern is that if the gaps at different waves are measured with tests that have different amounts of measurement error, then the amount of bias will not be the same in each measure of the gap. This can be very misleading and suggest erroneous intepretations regarding trends in the of the gaps over time \citet{reardon2011}. The PISA has tried to make sure the tests are comparable across waves (PISA 2012 technical report), but still we have to adjust for this imprecision.

In order to correct gap estimates for measurement error, I weight the student test scores by the P where P is the reliability of the test for each wave. Each PISA study provided a global reliability indicator which we use accordingly to each wave. This yields estimates of the true gaps, and eliminates any bias in the trend that may arise from differential reliability of the tests.

After constructing the adjusted test score measurement, we estimate the SES gap in a very simple fashion. For each country-wave, we estimate the weighted 90th and 10th percentile scores for the SES composite index, and then we generate a dummy of 1 for those above (including) the 90th percentile and 0 for those below (including) the 10th percentile. Finally, for each wave, we fit a mixed model with the adjusted test score as the dependent variable and the SES index as the only covariate. We allow both the intercept and the dummy variable to vary by country, and weight the model by the appropriate student weights.

We take the random effects of each country together with its standard error as the achievement gap. We fit a multilevel rather than a linear model because by pooling the information together, we weight countries appropriately to their sample size. Given that including the SES dummy reduces the sample size considerably, we want to be able to estimate each country-difference as accurately as possible.

% Show the properties of the ESCS dummy variable
% % in high educated
% % of home possessions
% % of ISEI

\section{Analysis}

\subsection{Descriptives}

% Figure out how to ouput data frames as latex tables.
<<sample_size, cache = TRUE>>=

# Get sample counts for each dummy
sample_size_calc <- function(df, probs, selected = F, cnts = NULL) {
  
  stopifnot(selected & !is.null(cnts))
  
  if (selected) df <- map(df, ~ filter(.x, country %in% cnts))
  
  cnt_to_bind <-
    map(df, function(df) {
      
      print(unique(df$wave))
      conf <- if (unique(df$wave) == "pisa2015") pisa2015_conf else pisa_conf
      weights_var <- conf$variables$weightFinal
      
      split_df <- split(df, df$country)
      
      split_df_two <-
        map(split_df, ~ {
          # In some countries the quan can't be estimated because of very few obs.
          # The function doesn't stop but returns two NA's.
          quan <- quantile_missing(.x, weights_var, probs)
          
          # It's very important to create a variable that returns the number of observations of this dummy
          # For each country. Possibly to weight by the number of observations.
          .x$escs_dummy <-
            with(.x, case_when(escs_trend >= quan[2] ~ 1,
                               escs_trend <= quan[1] ~ 0))
          .x
        })
      unsplit_df <- split_df_two %>% enframe() %>% unnest(value)
      
      unsplit_df %>%
        count(country, escs_dummy) %>%
        filter(!is.na(escs_dummy)) %>%
        left_join(summarize(group_by(unsplit_df, country), total_n = n()), by = "country") %>%
        mutate(perc = paste0(round(n / total_n * 100, 0), "%")) %>%
        select(-total_n)
    })
  setNames(cnt_to_bind, seq(2000, 2015, 3)) %>%
    enframe() %>%
    unnest()
}

sample_tables_topbottom <- sample_size_calc(adapted_year_data, c(.1, .9), selected = TRUE, countries)
sample_tables_topmid <- sample_size_calc(adapted_year_data, c(.5, .9), selected = TRUE, countries)
sample_tables_midbottom <- sample_size_calc(adapted_year_data, c(.1, .5), selected = TRUE, countries)
@

<<merge_math_read, cache = TRUE>>=

# Function does a lot of things, but in short:

# Calculate the difference between the gap and together with it's joint s.e
# Also uncertainty intervals and returns a tibble with the difference between
# SES gaps with the adjusted SE difference + uncertainty intervals + the original
# data (the absolute numbers before the differences)

pisa_preparer <- function(df_math, df_read) {

descrip_math <- map(df_math, ~ rename(.x, mean_math = Mean, se_math = s.e.))
descrip_read <- map(df_read, ~ rename(.x, mean_read = Mean, se_read = s.e.))


reduced_data_math <-
  map2(descrip_math, years, function(.x, .y) {
    .x %>%
      mutate(wave = .y) %>%
      filter(!is.na(escs_dummy))
  }) %>%
  bind_rows() %>%
  as_tibble() %>%
  mutate(lower_math = mean_math - 1.96 * se_math,
         upper_math = mean_math + 1.96 * se_math)

reduced_data_read <-
  map2(descrip_read, years, function(.x, .y) {
    .x %>%
      mutate(wave = .y) %>%
      filter(!is.na(escs_dummy))
  }) %>%
  bind_rows() %>%
  as_tibble() %>%
  mutate(lower_read = mean_read - 1.96 * se_read,
         upper_read = mean_read + 1.96 * se_read)

reduced_data <- left_join(reduced_data_math,
                          reduced_data_read, by = c("country", "escs_dummy", "wave"))

# Merging math and reading data
test_data <-
  reduced_data %>%
  select(country, wave, escs_dummy, contains("mean")) %>%
  gather(test, score, contains("mean"))

math_data <-
  reduced_data %>%
  select(country, wave, escs_dummy, contains("math")) %>%
  gather(test_bound, bound, contains("lower"), contains("upper")) %>%
  select(-contains("math")) %>%
  right_join(filter(test_data, test == "mean_math"))

read_data <-
  reduced_data %>%
  select(country, wave, escs_dummy, contains("read")) %>%
  gather(test_bound, bound, contains("lower"), contains("upper")) %>%
  select(-contains("read")) %>%
  right_join(filter(test_data, test == "mean_read"))

all_data <- bind_rows(math_data, read_data)

# Calculate the joint standard error of the difference
math_se_data <-
  reduced_data %>%
  select(country, escs_dummy, wave, se_math) %>%
  spread(escs_dummy, se_math) %>%
    transmute(country, wave,
              se_diff_math = sqrt(abs(`1`^2 - `0`^2)))

read_se_data <-
  reduced_data %>%
  select(country, escs_dummy, wave, se_read) %>%
  spread(escs_dummy, se_read) %>%
  transmute(country, wave,
            se_diff_read = sqrt(abs(`1`^2 - `0`^2)))

se_data <- left_join(math_se_data, read_se_data)

# Calculate the different between the gap and together with it's joint s.e graph
# the absolut difference.

math_diff <-
  reduced_data %>%
  select(wave, country, escs_dummy, mean_math) %>%
  spread(escs_dummy, mean_math) %>%
  transmute(wave, country, diff_math = `1` - `0`)

read_diff <-
  reduced_data %>%
  select(wave, country, escs_dummy, mean_read) %>%
  spread(escs_dummy, mean_read) %>%
  transmute(wave, country, diff_read = `1` - `0`)

data_summaries <-
  math_diff %>%
  left_join(read_diff) %>%
  left_join(se_data) %>%
  transmute(wave, country, diff_math, diff_read,
           lower_math = diff_math - 1.96 * se_diff_math,
           lower_read = diff_read - 1.96 * se_diff_read,
           upper_math = diff_math + 1.96 * se_diff_math,
           upper_read = diff_read + 1.96 * se_diff_read)

differences <-
  data_summaries %>%
  select(wave, country, diff_math, diff_read) %>%
  gather(test, difference, starts_with("diff")) %>%
  mutate(type_test = ifelse(.$test == "diff_math", "math", "read"))

bounds_lower <-
  data_summaries %>%
  select(wave, country, contains("lower")) %>%
  gather(lower_bound, lower, lower_math, lower_read) %>%
  mutate(type_test = ifelse(grepl("math", .$lower_bound), "math", "read"))

bounds_upper <-
  data_summaries %>%
  select(wave, country, contains("upper")) %>%
  gather(upper_bound, upper, upper_math, upper_read) %>%
  mutate(type_test = ifelse(grepl("math", .$upper_bound), "math", "read"))

# Getting the original data in
original_math <-
  reduced_data_math %>%
  select(wave, everything(), -se_math) %>%
  gather(metric, value, -(wave:escs_dummy)) %>%
  unite(combination, escs_dummy, metric, sep = "_") %>%
  spread(combination, value) %>%
  mutate(type_test = "math")

original_read <-
  reduced_data_read %>%
  select(wave, everything(), -se_read) %>%
  gather(metric, value, -(wave:escs_dummy)) %>%
  unite(combination, escs_dummy, metric, sep = "_") %>%
  spread(combination, value) %>%
  mutate(type_test = "read")

# final data
complete_data <-
  left_join(differences, bounds_lower) %>%
  left_join(bounds_upper) %>%
  left_join(original_math) %>%
  left_join(original_read)
}

complete_data_topbottom <- pisa_preparer(results_math, results_read)
complete_data_topmid <- pisa_preparer(results_math_topmid, results_read_topmid)
complete_data_midbottom <- pisa_preparer(results_math_midbottom, results_read_midbottom)

complete_data_topbottom <- mutate(complete_data_topbottom, type = "90th/10th SES gap")
complete_data_topmid <- mutate(complete_data_topmid, type = "90th/50th SES gap")
complete_data_midbottom <- mutate(complete_data_midbottom, type = "50th/10th SES gap")
@

\subsection{Results}

<<correlation_incomeineq, include = T, out.height = '5in', out.width = '5.5in', fig.align = 'center'>>=
complete_data_topbottom %>%
  mutate(wave = as.character(wave)) %>%
  left_join(inequalityintsvy::economic_inequality, by = c("wave" = "year", "country")) %>%
  filter(indicators == "GINI") %>%
  group_by(country) %>%
  summarize(avg_diff = mean(difference, na.rm = T),
            avg_value = mean(value, na.rm = T)) %>%
  filter(avg_value < 8) %>%
  ggplot(aes(avg_value, avg_diff)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ splines::ns(x, 2), linetype = "longdash", se = F)
@

<<graphing_9010gaps, include = T, out.height = '5in', out.width = '5.5in', fig.align = 'center'>>=

# 90/10 gaps acros countries
complete_data_topbottom %>%
  filter(country %in% c("United States", "Netherlands", "France",
                        "Germany", "Poland", "Finland")) %>%
  ggplot(aes(as.factor(wave), difference, group = type_test, colour = type_test)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.1) +
  geom_hline(yintercept = 0, linetype = "longdash") +
  geom_line() +
  geom_point(size = 0.5) +
  coord_cartesian(ylim = c(-0.5, 3)) +
  facet_wrap(~ country)
@

<<graphing_allgaps, include = T, out.height = '5in', out.width = '5.5in', fig.align = 'center'>>=

# Comparing all gaps across countries
complete_data_topbottom %>%
  bind_rows(complete_data_topmid) %>%
  bind_rows(complete_data_midbottom) %>%
  filter(country %in% c("United States", "Denmark", "France")) %>%
  mutate(type = factor(type,
                       levels = c("90th/10th SES gap", "90th/50th SES gap", "50th/10th SES gap"),
                       ordered = TRUE)) %>%
  ggplot(aes(as.factor(wave), difference, group = type_test, colour = type_test)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.1) +
  geom_hline(yintercept = 0, linetype = "longdash") +
  geom_line() +
  geom_point(size = 0.5) +
  coord_cartesian(ylim = c(-0.5, 3)) +
  facet_grid(country ~ type)
@

<<graphing_ses_growth, include = T, out.height = '5in', out.width = '5.5in', fig.align = 'center'>>=
# Graphing how the top/bottom are evolving over time instead of absolute difference

complete_data_topbottom %>%
  bind_rows(complete_data_topmid) %>%
  bind_rows(complete_data_midbottom) %>%
  filter(country %in% c("Germany", "Denmark", "France"), type_test == "math") %>%
  select(wave, country, type_test, type, contains("math")) %>%
  mutate(type = factor(type,
                       levels = c("90th/10th SES gap", "90th/50th SES gap", "50th/10th SES gap"),
                       ordered = TRUE)) %>%
  gather(score, value, -(wave:type)) %>%
  separate(score, c("ses", "score"), sep = 2) %>%
  spread(score, value) %>%
  ggplot(aes(as.factor(wave), mean_math, group = ses, colour = ses)) +
  geom_errorbar(aes(ymin = lower_math, ymax = upper_math), width = 0.1) +
  geom_hline(yintercept = 0, linetype = "longdash") +
  geom_line() +
  geom_point(size = 0.5) +
  coord_cartesian(ylim = c(-0.5, 3)) +
  facet_grid(country ~ type)

# Increase:  
# Sweden - steady increase in both tests
# Austria - increase in math - slight increase in read
# Finland - very sharp increase in both
# France - very sharp increase in both
# Netherlands - sharp increase in both

# Decrease:
# US - decrease in both tests
# Chile - decrease in both tests
  
# No change:
# Canada - stable red - increase math
# UK - slight decrease red - stable math
# Belgium - no change
# Czech republic - no change
# Denmark no change
# Germany - no change
# ITaly - no change
# Japan -  no change
# Norway - no change
# Poland - no change
# Spain - no change
@

<<graphing_achievement_disparity, cache = T>>=

country_scores <-
  map(adapted_year_data, ~ {
  if (unique(.x$wave) == "pisa2015") {
    intsvy::pisa2015.mean.pv("MATH", by = "country", data = .x)
  } else {
    intsvy::pisa.mean.pv("MATH", by = "country", data = .x)
  }
}) %>% setNames(seq(2000, 2015, 3))

country_scores <-
  enframe(country_scores, name = "wave") %>%
  unnest() %>%
  mutate(wave = as.double(wave))

data_to_plot <-
  left_join(complete_data_topbottom, country_scores, by = c("wave", "country")) %>%
  select(wave, country, type_test, difference, Mean) %>%
  filter(type_test == "math")

data_to_plot %>%
  ggplot(aes(scale(Mean), difference)) +
  geom_point() +
  geom_smooth()

data_to_plot %>%
  filter(country %in% countries) %>%
  ggplot(aes(as.character(wave), difference, group = country)) +
  geom_point(colour = "blue") +
  geom_line(colour = "blue") +
  geom_point(aes(y = scale(Mean)), colour = "red") +
  geom_line(aes(y = scale(Mean)), colour = "red") +
  facet_wrap(~ country)

data_to_plot %>%
  semi_join(data_to_plot %>% group_by(country) %>% count() %>% filter(n > 3), by = "country") %>%
  group_by(country) %>%
  summarize(cor = cor(scale(Mean), difference, use = 'complete.obs')) %>%
  ggplot(aes(reorder(country, cor), cor, fill = cor > 0)) + geom_col() + coord_flip()

@

% How to output data frames are latex tables?
<<rate_change, cache = TRUE>>=

avg_increase_fun <- function(df, class) {

# Average standard deviation increase
  data_ready <-
    df %>%
    select(wave, country, type_test, contains("mean_math")) %>%
    gather(metric, value, -(wave:type_test)) %>%
    separate(metric, c("ses", "test"), sep = 2) %>%
    spread(test, value) %>%
    mutate(ses = gsub("_", "", ses)) %>%
    filter(type_test == "math", ses == class) %>%
    split(.$country) %>%
    map(~ mutate(.,
                 diff = c(diff(mean_math, lag = 1), NA),
                 perc = round(diff / mean_math, 2) * 100,
                 perc_pos = mean(perc > 0, na.rm = T))) %>%
    enframe() %>%
    unnest(value) %>%
    split(.$country)
  
    map2(data_ready, names(data_ready), ~ {

      print(.y)
      
      mean_df <-
        bootstrapper(.x, mean(diff, na.rm = T), B = 500) %>%
        filter(type == "bootstrap") %>%
        rename(mean = value)

      sd_df <-
        bootstrapper(.x, sd(diff, na.rm = T), B = 500) %>%
        filter(type == "bootstrap") %>%
        rename(sd = value)
      
      suppressMessages(
        left_join(mean_df, sd_df) %>%
        mutate(lower_bound = mean - 1 * sd,
               upper_bound = mean + 1 * sd)
      )
    }) %>%
    enframe() %>%
    unnest(value)
}

avg_sd_increase_high <- avg_increase_fun(complete_data_topbottom, 1)
avg_sd_increase_low <- avg_increase_fun(complete_data_topbottom, 0)
@

<<rate_change_graph, include = T, out.height = '5in', out.width = '5.5in', fig.align = 'center'>>=

full_data <-
  left_join(select(avg_sd_increase_high, name, mean),
            select(avg_sd_increase_low, name, mean), by = "name") %>%
  mutate(continent = ifelse(name %in% countries, "my_cnt", "other_cnt"))

colnames(full_data) <- c("country", "high_increase", "low_increase", "continent")

lims <- list(xlim = c(-0.15, 0.25), ylim = c(-0.25, 0.25))

rect_data <- tibble(xst = c(lims$xlim[1], 0),
                    xen = c(0.0, lims$xlim[2]),
                    yst = c(0.0, lims$ylim[1]),
                    yen = c(lims$ylim[2], 0),
                    colour = c("red", "green"))

full_data %>%
  ggplot(aes(low_increase, high_increase), alpha = 0.2) +
  geom_rect(data = rect_data, aes(xmin = xst,
                                  xmax = xen,
                                  ymin = yst,
                                  ymax = yen),
            fill = rect_data$colour,
            alpha = 0.2,
            inherit.aes = FALSE) +
  geom_line(stat="smooth", method = "lm", se = FALSE, alpha = 0.5, colour = "grey", size = 1) +
  geom_point(alpha = 0.2) +
  geom_point(data = filter(full_data, continent == "my_cnt"), colour = "red", alpha = 0.7) +
  geom_text_repel(data = filter(full_data, continent == "my_cnt"),
                  aes(label = country), box.padding = unit(2.7, "lines")) +
  geom_vline(xintercept = 0, alpha = 0.5) +
  geom_hline(yintercept = 0, alpha = 0.5) +
  xlim(lims$xlim) +
  ylim(lims$ylim) +
  coord_cartesian(expand = FALSE) +
  annotate(geom = "text", x = 0.15, y = -0.2,
           label = "Low SES are catching up \n faster than High SES",
           fontface = 2, size = 3) +
  annotate(geom = "text", x = -0.05, y = 0.20,
           label = "High SES are increasing  \n faster than Low SES",
           fontface = 2, size = 3) +
  labs(x = "Average increase of low SES in SD", y = "Average increase of high SES in SD") +
  theme_minimal()
@

% How to output data frames are latex tables?
<<perc_increase_tables, cache = TRUE>>=
# Show the rates at which is increasing/decreasing
perc_increase_fun <- function(df) {
  
  # Average standard deviation increase
  data_ready <-
    df %>%
    select(wave, country, type_test, difference) %>%
    group_by(type_test) %>%
    split(.$country) %>%
    map(~ {
      .x <-
        spread(.x, wave, difference) %>%
        ungroup()
  
      year_vars <- sum(map_dbl(.x, is.numeric)) - 1
      years_subtract <- names(.x)[c(ncol(.x) - year_vars, ncol(.x))]
      years_subtract <- lapply(years_subtract, as.name)

      last_year <- rlang::new_quosure(years_subtract[[2]], env = .GlobalEnv)
      first_year <- rlang::new_quosure(years_subtract[[1]], env = .GlobalEnv)
      
      years_available <-
        .x %>%
        gather(year, val, -(country:type_test)) %>%
        group_by(type_test) %>%
        summarise(yr_avaible = sum(!is.na(val))) %>%
        pull(yr_avaible)
      
      year_sd <-
        .x %>%
        gather(year, val, -(country:type_test)) %>%
        split(.$type_test) %>%
        map_dbl(~ bootstrapper(.x, mad(val, na.rm = T), B = 100) %>% .[[2, 2]]) %>%
        round(2) * 100

      
      .x %>%
        map_if(is_double, round, 3) %>%
        as_tibble() %>%
        transmute(type_test,
                  country,
                  perc_diff = round(((!!last_year) - (!!first_year)) * 100, 1),
                  sd_year = year_sd,
                  diff_lower = perc_diff - 1 * year_sd,
                  diff_upper = perc_diff + 1 * year_sd,
                  years_available = years_available)
    })
  data_ready
}

top_bottom_perc <- perc_increase_fun(complete_data_topbottom)
top_mid_perc <- perc_increase_fun(complete_data_topmid)
mid_bottom_perc <- perc_increase_fun(complete_data_midbottom)

# Gap is closing at an average of the variable diff per year.
@

<<graphing_perc_increase, include = T, out.height = '5in', out.width = '5.5in', fig.align = 'center'>>=
perc_graph <- function(df) {
  df %>%
  enframe(name = "country") %>%
  unnest(value) %>%
  filter(country %in% countries) %>%
  select(-country1, -years_available) %>%
  mutate(diff_95_lower = perc_diff - 2*sd_year,
         diff_95_upper = perc_diff + 2*sd_year) %>%
  setNames(c("Country", "Type of test", "Average % difference", "Average SD",
             "Lower 50% bound", "Upper 50% bound", "Lower 95% bound", "Upper 95% bound")) %>%
  filter(`Type of test` == "math") %>%
  arrange(`Average % difference`) %>%
  mutate(Country = ordered(forcats::as_factor(Country)),
         Country_num = as.numeric(Country)) %>%
  ggplot(aes(Country, `Average % difference`)) +
  geom_point() +
  geom_ribbon(aes(x = Country_num, ymin = `Lower 95% bound`, ymax = `Upper 95% bound`), alpha = 0.5) +
  geom_ribbon(aes(x = Country_num, ymin = `Lower 50% bound`, ymax = `Upper 50% bound`), alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "longdash") +
  scale_y_continuous(breaks = seq(-160, 160, 40)) +
  coord_cartesian(ylim = c(-160, 160)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

m1 <- perc_graph(top_bottom_perc)
m2 <- perc_graph(top_mid_perc)
m3 <- perc_graph(mid_bottom_perc)

gridExtra::grid.arrange(m1, m2, m3, nrow = 1)
@

<<table_sample>>=
sample_tables_topbottom %>%
  filter(name %in% c(2000, 2015)) %>%
  transmute(name, country, escs_dummy, N = paste0(n, " (", perc, ")")) %>%
  spread(escs_dummy, N) %>%
  rename(Year = name,
         Country = country,
         `10th SES` = `0`,
         `90th SES` = `1`) %>%
  select(Country, Year, everything()) %>%
  arrange(Country, Year) %>%
  xtable::xtable()
@

<<next_steps>>=
# Next steps:

# Continue by doing the multilevel models to see what explains what. Include
# all indicators from the reardon/russian girl paper.
  
# Graph the increase in each country vs the increase/decrease of the economic inequality indicators
# Specially the 90/10

# Calculate how big is the gap between reading and math
  
# Continue with the PIRLS to see if there are specific patterns in 4th and 8th graders gap.
  
# Get each country trendline adjusted for the inequality indicators and place in the same country graph.

# Should I add the parent's education in the lm model to see how trends change adjusted for that?

# A weak welfare system, together with income inequality, what's their pattern?
# What if we put the school differentiation/tracking aspect in? Are there country groups based on these
# patterns.

# In countries where there is high differentiation/tracking, is there a jump in the evolution of the gap between PIRLS/TIMSS and PISA?
@

\bibliography{mybibliography.bib}

\end{document}
